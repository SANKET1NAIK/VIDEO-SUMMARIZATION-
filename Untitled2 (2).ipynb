{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85cafcc7-34e7-4596-b8dd-8a4a755f7111",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 317)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:317\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"L_F_Hip\": kp_array[5][:2],\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "import os\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Ensure CUDA is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. Please check your GPU and PyTorch installation.\")\n",
    "\n",
    "class CombinedVisualizer:\n",
    "    def __init__(self, window_size=100):\n",
    "        self.window_size = window_size\n",
    "        self.activities = deque(maxlen=window_size)\n",
    "        self.activity_counts = {\n",
    "            \"walking\": 0,\n",
    "            \"standing\": 0,\n",
    "            \"sitting\": 0,\n",
    "            \"pawing\": 0,\n",
    "            \"Unknown\": 0,\n",
    "        }\n",
    "        self.total_frames = 0\n",
    "\n",
    "    def update(self, activity):\n",
    "        self.activities.append(activity)\n",
    "        self.activity_counts[activity] += 1\n",
    "        self.total_frames += 1\n",
    "\n",
    "    def create_visualization(self, frame):\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        defrag_height = 150\n",
    "        defrag_width = frame_width\n",
    "        defrag_image = np.ones((defrag_height, defrag_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        colors = {\n",
    "            \"walking\": (0, 0, 255),  # Red\n",
    "            \"standing\": (255, 0, 0),  # Blue\n",
    "            \"sitting\": (0, 255, 0),  # Green\n",
    "            \"pawing\": (255, 165, 0),  # Orange\n",
    "            \"Unknown\": (128, 128, 128),  # Gray\n",
    "        }\n",
    "\n",
    "        segment_width = defrag_width // self.window_size\n",
    "        for i, activity in enumerate(self.activities):\n",
    "            x_start = i * segment_width\n",
    "            x_end = x_start + segment_width\n",
    "            color = colors.get(activity, (128, 128, 128))\n",
    "            cv2.rectangle(defrag_image, (x_start, 0), (x_end, 80), color, -1)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        legend_items = [\n",
    "            (\"Walking\", (0, 0, 255)),\n",
    "            (\"Standing\", (255, 0, 0)),\n",
    "            (\"Sitting\", (0, 255, 0)),\n",
    "            (\"Pawing\", (255, 165, 0)),\n",
    "            (\"Unknown\", (128, 128, 128)),\n",
    "        ]\n",
    "\n",
    "        x_offset = 10\n",
    "        y_offset = 120\n",
    "        for text, color in legend_items:\n",
    "            cv2.rectangle(\n",
    "                defrag_image,\n",
    "                (x_offset, y_offset - 15),\n",
    "                (x_offset + 20, y_offset + 5),\n",
    "                color,\n",
    "                -1,\n",
    "            )\n",
    "            percentage = (\n",
    "                self.activity_counts.get(text.lower(), 0) / max(1, self.total_frames)\n",
    "            ) * 100\n",
    "            cv2.putText(\n",
    "                defrag_image,\n",
    "                f\"{text}: {percentage:.1f}%\",\n",
    "                (x_offset + 30, y_offset),\n",
    "                font,\n",
    "                0.5,\n",
    "                (0, 0, 0),\n",
    "                1,\n",
    "            )\n",
    "            x_offset += 160\n",
    "\n",
    "        combined_height = frame_height + defrag_height\n",
    "        combined_image = np.zeros((combined_height, frame_width, 3), dtype=np.uint8)\n",
    "        combined_image[:frame_height] = frame\n",
    "        combined_image[frame_height:] = defrag_image\n",
    "\n",
    "        return combined_image\n",
    "\n",
    "class TensorRTInference:\n",
    "    \"\"\"TensorRT inference class for ViTPose model, optimized for GPU with TensorRT 10.8.0.43\"\"\"\n",
    "    def __init__(self, engine_path):\n",
    "        \"\"\"Initialize TensorRT engine and allocate buffers on GPU.\"\"\"\n",
    "        self.logger = trt.Logger(trt.Logger.INFO)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "\n",
    "        # Load engine from file\n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_bytes = f.read()\n",
    "            self.engine = self.runtime.deserialize_cuda_engine(engine_bytes)\n",
    "\n",
    "        if not self.engine:\n",
    "            raise RuntimeError(f\"Failed to load TensorRT engine from {engine_path}\")\n",
    "\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Define input and output shapes\n",
    "        self.input_shape = (1, 3, 256, 192)  # ViTPose standard input\n",
    "        self.output_shape = (1, 17, 64, 48)  # ViTPose standard output (17 keypoints)\n",
    "        \n",
    "        # Tensor names (adjust based on your engine)\n",
    "        self.input_name = \"input\"\n",
    "        self.output_name = \"output\"\n",
    "        \n",
    "        # Allocate GPU buffers\n",
    "        self.allocate_buffers()\n",
    "\n",
    "    def allocate_buffers(self):\n",
    "        \"\"\"Allocate CUDA memory for inputs and outputs.\"\"\"\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.bindings = []\n",
    "\n",
    "        num_io_tensors = self.engine.num_io_tensors\n",
    "        for i in range(num_io_tensors):\n",
    "            tensor_name = self.engine.get_tensor_name(i)\n",
    "            tensor_shape = self.engine.get_tensor_shape(tensor_name)\n",
    "            tensor_dtype = trt.nptype(self.engine.get_tensor_dtype(tensor_name))\n",
    "            \n",
    "            if tensor_dtype == np.float32:\n",
    "                torch_dtype = torch.float32\n",
    "            elif tensor_dtype == np.float16:\n",
    "                torch_dtype = torch.float16\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported dtype: {tensor_dtype}\")\n",
    "\n",
    "            # Explicitly allocate on GPU (CUDA)\n",
    "            tensor = torch.zeros(tuple(tensor_shape), dtype=torch_dtype, device='cuda')\n",
    "            self.bindings.append(tensor.data_ptr())\n",
    "            \n",
    "            if self.engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:\n",
    "                self.inputs.append(tensor)\n",
    "            else:\n",
    "                self.outputs.append(tensor)\n",
    "\n",
    "            self.context.set_tensor_address(tensor_name, tensor.data_ptr())\n",
    "\n",
    "    def preprocess_image(self, img):\n",
    "        \"\"\"Preprocess image for model input on GPU.\"\"\"\n",
    "        img = cv2.resize(img, (192, 256))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = img.transpose(2, 0, 1)  # HWC to CHW\n",
    "        img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "        \n",
    "        # Move to GPU\n",
    "        return torch.from_numpy(img).to('cuda')\n",
    "\n",
    "    def postprocess_output(self, output):\n",
    "        \"\"\"Convert model output to keypoints on GPU, then move to CPU for further processing.\"\"\"\n",
    "        heatmaps = output.reshape(self.output_shape)\n",
    "        keypoints = {}\n",
    "        for batch_idx in range(heatmaps.shape[0]):\n",
    "            person_keypoints = torch.zeros((17, 3), device='cuda')  # Process on GPU\n",
    "            for kpt_idx in range(17):\n",
    "                heatmap = heatmaps[batch_idx, kpt_idx]\n",
    "                flat_idx = torch.argmax(heatmap)\n",
    "                y, x = torch.unravel_index(flat_idx, heatmap.shape)\n",
    "                orig_x = x * (192 / 48)  # Scale to input width\n",
    "                orig_y = y * (256 / 64)  # Scale to input height\n",
    "                confidence = heatmap[y, x]\n",
    "                person_keypoints[kpt_idx] = torch.tensor([orig_x, orig_y, confidence], device='cuda')\n",
    "            keypoints[batch_idx] = person_keypoints.cpu().numpy()  # Move to CPU for compatibility\n",
    "        return keypoints\n",
    "    \n",
    "    def infer(self, img):\n",
    "        \"\"\"Run inference on an image using GPU.\"\"\"\n",
    "        preprocessed = self.preprocess_image(img)\n",
    "        self.inputs[0].copy_(preprocessed)\n",
    "        \n",
    "        # Execute inference asynchronously on GPU\n",
    "        self.context.execute_async_v3(stream_handle=torch.cuda.current_stream().cuda_stream)\n",
    "        torch.cuda.synchronize()  # Wait for GPU computation to finish\n",
    "        \n",
    "        output = self.outputs[0]  # Already on GPU\n",
    "        keypoints = self.postprocess_output(output)\n",
    "        return keypoints\n",
    "\n",
    "class HorseGaitEnv(gym.Env):\n",
    "    \"\"\"Custom Gym environment for horse gait RL.\"\"\"\n",
    "    def __init__(self, monitor):\n",
    "        super(HorseGaitEnv, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.action_space = spaces.Discrete(5)  # 5 actions: walking, standing, sitting, pawing, unknown\n",
    "        # State space: keypoints (17 * 3), movement buffer, front paw history\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(17*3 + 10 + 30*2,), dtype=np.float32)\n",
    "        self.current_keypoints = None\n",
    "        self.frame_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.monitor.prev_positions = None\n",
    "        self.monitor.movement_buffer.clear()\n",
    "        self.monitor.front_paw_positions_history.clear()\n",
    "        self.frame_count = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        predicted_state = [\"walking\", \"standing\", \"sitting\", \"pawing\", \"unknown\"][action]\n",
    "        heuristic_state = self.monitor.detect_state(self.current_keypoints)  # Use existing heuristic\n",
    "        \n",
    "        # Reward function\n",
    "        reward = 1.0 if predicted_state == heuristic_state else -0.5\n",
    "        \n",
    "        # Additional reward shaping for pawing\n",
    "        if predicted_state == \"pawing\" and self.monitor.detect_pawing_pattern(\n",
    "            [self.current_keypoints[0][7][:2], self.current_keypoints[0][10][:2]],  # Front paws\n",
    "            [self.monitor.calculate_angle(*[np.array(self.current_keypoints[0][i][:2]) for i in [5, 6, 7]]),\n",
    "             self.monitor.calculate_angle(*[np.array(self.current_keypoints[0][i][:2]) for i in [8, 9, 10]])])\n",
    "        ):\n",
    "            reward += 2.0  # Bonus for correct pawing detection\n",
    "        \n",
    "        self.monitor.visualizer.update(predicted_state)\n",
    "        self.frame_count += 1\n",
    "        \n",
    "        done = self.frame_count >= 1000  # Arbitrary episode length\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_keypoints is None:\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        \n",
    "        keypoints_flat = self.current_keypoints[0].flatten()  # 17 keypoints * 3 (x, y, conf)\n",
    "        movement_buffer = np.array(self.monitor.movement_buffer + [0] * (10 - len(self.monitor.movement_buffer)))\n",
    "        paw_history = np.array(list(self.monitor.front_paw_positions_history) + [[0, 0]] * (30 - len(self.monitor.front_paw_positions_history))).flatten()\n",
    "        return np.concatenate([keypoints_flat, movement_buffer, paw_history])\n",
    "\n",
    "class HorseGaitMonitor:\n",
    "    def __init__(self, model_path, output_dir=\"monitoring_output\"):\n",
    "        \"\"\"Initialize the horse gait monitoring system with TensorRT on GPU and RL.\"\"\"\n",
    "        self.model = TensorRTInference(model_path)\n",
    "        \n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        self.pose_dirs = {\n",
    "            \"standing\": os.path.join(output_dir, \"standing\"),\n",
    "            \"walking\": os.path.join(output_dir, \"walking\"),\n",
    "            \"sitting\": os.path.join(output_dir, \"sitting\"),\n",
    "            \"pawing\": os.path.join(output_dir, \"pawing\"),\n",
    "        }\n",
    "        for dir_path in self.pose_dirs.values():\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        self.prev_positions = None\n",
    "        self.movement_buffer = deque(maxlen=10)\n",
    "        self.state_buffer = deque(maxlen=15)\n",
    "        self.visualizer = CombinedVisualizer()\n",
    "        \n",
    "        # New variables for improved pawing detection\n",
    "        self.front_paw_positions_history = deque(maxlen=30)\n",
    "        self.front_leg_angles_history = deque(maxlen=30)\n",
    "        self.pawing_pattern_count = 0\n",
    "        self.pawing_cooldown = 0\n",
    "        self.pawing_detection_threshold = 3\n",
    "        self.pawing_angle_threshold = (100, 140)\n",
    "\n",
    "        # RL Integration\n",
    "        self.env = HorseGaitEnv(self)\n",
    "        self.rl_model = DQN(\"MlpPolicy\", DummyVecEnv([lambda: self.env]), verbose=1, device=\"cuda\")\n",
    "        self.is_training = True  # Toggle training mode\n",
    "\n",
    "    def calculate_angle(self, p1, p2, p3):\n",
    "        \"\"\"Calculate angle between three points (CPU-based for simplicity).\"\"\"\n",
    "        v1 = p1 - p2\n",
    "        v2 = p3 - p2\n",
    "        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        angle = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))\n",
    "        return angle\n",
    "\n",
    "    def detect_pawing_pattern(self, front_paw_positions, front_leg_angles):\n",
    "        \"\"\"Detect pawing pattern by analyzing the history of front paw movements and leg angles.\"\"\"\n",
    "        self.front_paw_positions_history.append(front_paw_positions)\n",
    "        self.front_leg_angles_history.append(front_leg_angles)\n",
    "        \n",
    "        if len(self.front_paw_positions_history) < 10:\n",
    "            return False\n",
    "            \n",
    "        for leg_idx in range(2):\n",
    "            y_positions = np.array([pos[leg_idx][1] for pos in self.front_paw_positions_history])\n",
    "            y_movements = np.diff(y_positions)\n",
    "            direction_changes = np.sign(y_movements[1:]) != np.sign(y_movements[:-1])\n",
    "            num_direction_changes = np.sum(direction_changes)\n",
    "            \n",
    "            angles = np.array([angles[leg_idx] for angles in self.front_leg_angles_history])\n",
    "            \n",
    "            if num_direction_changes >= 3:\n",
    "                vertical_range = np.max(y_positions) - np.min(y_positions)\n",
    "                angles_in_range = np.any((angles >= self.pawing_angle_threshold[0]) & \n",
    "                                        (angles <= self.pawing_angle_threshold[1]))\n",
    "                \n",
    "                if vertical_range > 15 and angles_in_range:\n",
    "                    return True\n",
    "                    \n",
    "        return False\n",
    "\n",
    "    def detect_state(self, keypoints):\n",
    "        \"\"\"Enhanced detect_state method with improved pawing detection (used for heuristic rewards).\"\"\"\n",
    "        movement_detected = False\n",
    "\n",
    "        for person_id, kp_array in keypoints.items():\n",
    "            leg_points = {\n",
    "                \"L_F_Hip\": kp_array[5][:2],\n",
    "                \"L_F_Knee\": kp_array[6][:2],\n",
    "                \"L_F_Paw\": kp_array[7][:2],\n",
    "                \"R_F_Hip\": kp_array[8][:2],\n",
    "                \"R_F_Knee\": kp_array[9][:2],\n",
    "                \"R_F_Paw\": kp_array[10][:2],\n",
    "                \"L_B_Hip\": kp_array[11][:2],\n",
    "                \"L_B_Knee\": kp_array[12][:2],\n",
    "                \"L_B_Paw\": kp_array[13][:2],\n",
    "                \"R_B_Hip\": kp_array[14][:2],\n",
    "                \"R_B_Knee\": kp_array[15][:2],\n",
    "                \"R_B_Paw\": kp_array[16][:2]\n",
    "            }\n",
    "\n",
    "            angles = {\n",
    "                \"left_front\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"L_F_Hip\"]),\n",
    "                    np.array(leg_points[\"L_F_Knee\"]),\n",
    "                    np.array(leg_points[\"L_F_Paw\"])\n",
    "                ),\n",
    "                \"right_front\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"R_F_Hip\"]),\n",
    "                    np.array(leg_points[\"R_F_Knee\"]),\n",
    "                    np.array(leg_points[\"R_F_Paw\"])\n",
    "                ),\n",
    "                \"left_back\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"L_B_Hip\"]),\n",
    "                    np.array(leg_points[\"L_B_Knee\"]),\n",
    "                    np.array(leg_points[\"L_B_Paw\"])\n",
    "                ),\n",
    "                \"right_back\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"R_B_Hip\"]),\n",
    "                    np.array(leg_points[\"R_B_Knee\"]),\n",
    "                    np.array(leg_points[\"R_B_Paw\"])\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            front_paw_positions = [leg_points[\"L_F_Paw\"], leg_points[\"R_F_Paw\"]]\n",
    "            front_leg_angles = [angles[\"left_front\"], angles[\"right_front\"]]\n",
    "            \n",
    "            if self.pawing_cooldown > 0:\n",
    "                self.pawing_cooldown -= 1\n",
    "                return \"pawing\"\n",
    "                \n",
    "            if self.detect_pawing_pattern(front_paw_positions, front_leg_angles):\n",
    "                self.pawing_pattern_count += 1\n",
    "                if self.pawing_pattern_count >= self.pawing_detection_threshold:\n",
    "                    self.pawing_pattern_count = 0\n",
    "                    self.pawing_cooldown = 15\n",
    "                    return \"pawing\"\n",
    "            else:\n",
    "                self.pawing_pattern_count = max(0, self.pawing_pattern_count - 0.5)\n",
    "            \n",
    "            if all(angle < 90 for angle in angles.values()):\n",
    "                return \"sitting\"\n",
    "\n",
    "            paw_positions = np.array([\n",
    "                leg_points[\"L_F_Paw\"],\n",
    "                leg_points[\"R_F_Paw\"],\n",
    "                leg_points[\"L_B_Paw\"],\n",
    "                leg_points[\"R_B_Paw\"]\n",
    "            ])\n",
    "\n",
    "            if self.prev_positions is not None:\n",
    "                movements = np.linalg.norm(paw_positions - self.prev_positions, axis=1)\n",
    "                self.movement_buffer.append(np.mean(movements))\n",
    "                if len(self.movement_buffer) > 10:\n",
    "                    self.movement_buffer.pop(0)\n",
    "                movement_detected = np.mean(self.movement_buffer) > 5.0\n",
    "\n",
    "            self.prev_positions = paw_positions\n",
    "\n",
    "        current_state = \"walking\" if movement_detected else \"standing\"\n",
    "        self.state_buffer.append(current_state)\n",
    "        if len(self.state_buffer) > 15:\n",
    "            self.state_buffer.pop(0)\n",
    "        return max(set(self.state_buffer), key=self.state_buffer.count)\n",
    "\n",
    "    def draw_state_annotation(self, frame, state):\n",
    "        \"\"\"Draw state annotation on frame without detailed visualization.\"\"\"\n",
    "        return frame\n",
    "\n",
    "    def save_frame(self, frame, state, frame_count):\n",
    "        \"\"\"Save frame to appropriate directory based on state.\"\"\"\n",
    "        if state in self.pose_dirs:\n",
    "            filename = os.path.join(self.pose_dirs[state], f\"frame_{frame_count}.jpg\")\n",
    "            cv2.imwrite(filename, frame)\n",
    "\n",
    "    def draw_keypoints(self, frame, keypoints):\n",
    "        \"\"\"Return original frame without keypoints drawing.\"\"\"\n",
    "        return frame\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        \"\"\"Process video file and analyze horse gait on GPU with RL.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        combined_height = height + 150\n",
    "        output_path = os.path.join(self.output_dir, \"video_with_analysis_rl.mp4\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, combined_height))\n",
    "\n",
    "        frame_count = 0\n",
    "        obs = self.env.reset()\n",
    "\n",
    "        cv2.namedWindow(\"Horse Gait Analysis\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                keypoints = self.model.infer(frame)\n",
    "                self.env.current_keypoints = keypoints\n",
    "\n",
    "                # RL Prediction\n",
    "                action, _ = self.rl_model.predict(obs, deterministic=not self.is_training)\n",
    "                current_state = [\"walking\", \"standing\", \"sitting\", \"pawing\", \"unknown\"][action]\n",
    "                \n",
    "                obs, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                if self.is_training:\n",
    "                    self.rl_model.learn(total_timesteps=1, reset_num_timesteps=False)\n",
    "\n",
    "                annotated_frame = frame.copy()\n",
    "                self.save_frame(annotated_frame, current_state, frame_count)\n",
    "\n",
    "                combined_display = self.visualizer.create_visualization(annotated_frame)\n",
    "                cv2.imshow(\"Horse Gait Analysis\", combined_display)\n",
    "                out.write(combined_display)\n",
    "\n",
    "                frame_count += 1\n",
    "                if done:\n",
    "                    obs = self.env.reset()\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "\n",
    "        finally:\n",
    "            cap.release()\n",
    "            out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(f\"\\nProcessing complete! Output saved to: {output_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the horse gait analysis on GPU with RL.\"\"\"\n",
    "    engine_path = \"C:/Users/hp/Downloads/vitpose-l-ap10k.engine\"\n",
    "    try:\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        monitor = HorseGaitMonitor(engine_path)\n",
    "        monitor.process_video(\"E:/vitpose/strach.mp4\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb35f30-e087-4c91-a0f5-6f316292f30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "\n",
      "Processing complete! Output saved to: monitoring_output\\video_with_analysis2.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# Ensure CUDA is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. Please check your GPU and PyTorch installation.\")\n",
    "\n",
    "class CombinedVisualizer:\n",
    "    def __init__(self, window_size=100):\n",
    "        self.window_size = window_size\n",
    "        self.activities = deque(maxlen=window_size)\n",
    "        self.activity_counts = {\n",
    "            \"walking\": 0,\n",
    "            \"standing\": 0,\n",
    "            \"sitting\": 0,\n",
    "            \"pawing\": 0,\n",
    "            \"Unknown\": 0,\n",
    "        }\n",
    "        self.total_frames = 0\n",
    "\n",
    "    def update(self, activity):\n",
    "        self.activities.append(activity)\n",
    "        self.activity_counts[activity] += 1\n",
    "        self.total_frames += 1\n",
    "\n",
    "    def create_visualization(self, frame):\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        defrag_height = 150\n",
    "        defrag_width = frame_width\n",
    "        defrag_image = np.ones((defrag_height, defrag_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        colors = {\n",
    "            \"walking\": (0, 0, 255),  # Red\n",
    "            \"standing\": (255, 0, 0),  # Blue\n",
    "            \"sitting\": (0, 255, 0),  # Green\n",
    "            \"pawing\": (255, 165, 0),  # Orange\n",
    "            \"Unknown\": (128, 128, 128),  # Gray\n",
    "        }\n",
    "\n",
    "        segment_width = defrag_width // self.window_size\n",
    "        for i, activity in enumerate(self.activities):\n",
    "            x_start = i * segment_width\n",
    "            x_end = x_start + segment_width\n",
    "            color = colors.get(activity, (128, 128, 128))\n",
    "            cv2.rectangle(defrag_image, (x_start, 0), (x_end, 80), color, -1)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        legend_items = [\n",
    "            (\"Walking\", (0, 0, 255)),\n",
    "            (\"Standing\", (255, 0, 0)),\n",
    "            (\"Sitting\", (0, 255, 0)),\n",
    "            (\"Pawing\", (255, 165, 0)),\n",
    "            (\"Unknown\", (128, 128, 128)),\n",
    "        ]\n",
    "\n",
    "        x_offset = 10\n",
    "        y_offset = 120\n",
    "        for text, color in legend_items:\n",
    "            cv2.rectangle(\n",
    "                defrag_image,\n",
    "                (x_offset, y_offset - 15),\n",
    "                (x_offset + 20, y_offset + 5),\n",
    "                color,\n",
    "                -1,\n",
    "            )\n",
    "            percentage = (\n",
    "                self.activity_counts.get(text.lower(), 0) / max(1, self.total_frames)\n",
    "            ) * 100\n",
    "            cv2.putText(\n",
    "                defrag_image,\n",
    "                f\"{text}: {percentage:.1f}%\",\n",
    "                (x_offset + 30, y_offset),\n",
    "                font,\n",
    "                0.5,\n",
    "                (0, 0, 0),\n",
    "                1,\n",
    "            )\n",
    "            x_offset += 160\n",
    "\n",
    "        combined_height = frame_height + defrag_height\n",
    "        combined_image = np.zeros((combined_height, frame_width, 3), dtype=np.uint8)\n",
    "        combined_image[:frame_height] = frame\n",
    "        combined_image[frame_height:] = defrag_image\n",
    "\n",
    "        return combined_image\n",
    "\n",
    "class TensorRTInference:\n",
    "    \"\"\"TensorRT inference class for ViTPose model, optimized for GPU with TensorRT 10.8.0.43\"\"\"\n",
    "    def __init__(self, engine_path):\n",
    "        \"\"\"Initialize TensorRT engine and allocate buffers on GPU.\"\"\"\n",
    "        self.logger = trt.Logger(trt.Logger.INFO)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "\n",
    "        # Load engine from file\n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_bytes = f.read()\n",
    "            self.engine = self.runtime.deserialize_cuda_engine(engine_bytes)\n",
    "\n",
    "        if not self.engine:\n",
    "            raise RuntimeError(f\"Failed to load TensorRT engine from {engine_path}\")\n",
    "\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Define input and output shapes\n",
    "        self.input_shape = (1, 3, 256, 192)  # ViTPose standard input\n",
    "        self.output_shape = (1, 17, 64, 48)  # ViTPose standard output (17 keypoints)\n",
    "        \n",
    "        # Tensor names (adjust based on your engine)\n",
    "        self.input_name = \"input\"\n",
    "        self.output_name = \"output\"\n",
    "        \n",
    "        # Allocate GPU buffers\n",
    "        self.allocate_buffers()\n",
    "\n",
    "    def allocate_buffers(self):\n",
    "        \"\"\"Allocate CUDA memory for inputs and outputs.\"\"\"\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.bindings = []\n",
    "\n",
    "        num_io_tensors = self.engine.num_io_tensors\n",
    "        for i in range(num_io_tensors):\n",
    "            tensor_name = self.engine.get_tensor_name(i)\n",
    "            tensor_shape = self.engine.get_tensor_shape(tensor_name)\n",
    "            tensor_dtype = trt.nptype(self.engine.get_tensor_dtype(tensor_name))\n",
    "            \n",
    "            if tensor_dtype == np.float32:\n",
    "                torch_dtype = torch.float32\n",
    "            elif tensor_dtype == np.float16:\n",
    "                torch_dtype = torch.float16\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported dtype: {tensor_dtype}\")\n",
    "\n",
    "            # Explicitly allocate on GPU (CUDA)\n",
    "            tensor = torch.zeros(tuple(tensor_shape), dtype=torch_dtype, device='cuda')\n",
    "            self.bindings.append(tensor.data_ptr())\n",
    "            \n",
    "            if self.engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:\n",
    "                self.inputs.append(tensor)\n",
    "            else:\n",
    "                self.outputs.append(tensor)\n",
    "\n",
    "            self.context.set_tensor_address(tensor_name, tensor.data_ptr())\n",
    "\n",
    "    def preprocess_image(self, img):\n",
    "        \"\"\"Preprocess image for model input on GPU.\"\"\"\n",
    "        # Convert to RGB and resize on CPU first (cv2 doesn't support GPU natively)\n",
    "        img = cv2.resize(img, (192, 256))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = img.transpose(2, 0, 1)  # HWC to CHW\n",
    "        img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "        \n",
    "        # Move to GPU\n",
    "        return torch.from_numpy(img).to('cuda')\n",
    "\n",
    "    def postprocess_output(self, output):\n",
    "        \"\"\"Convert model output to keypoints on GPU, then move to CPU for further processing.\"\"\"\n",
    "        heatmaps = output.reshape(self.output_shape)\n",
    "        keypoints = {}\n",
    "        for batch_idx in range(heatmaps.shape[0]):\n",
    "            person_keypoints = torch.zeros((17, 3), device='cuda')  # Process on GPU\n",
    "            for kpt_idx in range(17):\n",
    "                heatmap = heatmaps[batch_idx, kpt_idx]\n",
    "                flat_idx = torch.argmax(heatmap)\n",
    "                y, x = torch.unravel_index(flat_idx, heatmap.shape)\n",
    "                orig_x = x * (192 / 48)  # Scale to input width\n",
    "                orig_y = y * (256 / 64)  # Scale to input height\n",
    "                confidence = heatmap[y, x]\n",
    "                person_keypoints[kpt_idx] = torch.tensor([orig_x, orig_y, confidence], device='cuda')\n",
    "            keypoints[batch_idx] = person_keypoints.cpu().numpy()  # Move to CPU for compatibility\n",
    "        return keypoints\n",
    "    \n",
    "    def infer(self, img):\n",
    "        \"\"\"Run inference on an image using GPU.\"\"\"\n",
    "        preprocessed = self.preprocess_image(img)\n",
    "        self.inputs[0].copy_(preprocessed)\n",
    "        \n",
    "        # Execute inference asynchronously on GPU\n",
    "        self.context.execute_async_v3(stream_handle=torch.cuda.current_stream().cuda_stream)\n",
    "        torch.cuda.synchronize()  # Wait for GPU computation to finish\n",
    "        \n",
    "        output = self.outputs[0]  # Already on GPU\n",
    "        keypoints = self.postprocess_output(output)\n",
    "        return keypoints\n",
    "\n",
    "class HorseGaitMonitor:\n",
    "    def __init__(self, model_path, output_dir=\"monitoring_output\"):\n",
    "        \"\"\"Initialize the horse gait monitoring system with TensorRT on GPU.\"\"\"\n",
    "        self.model = TensorRTInference(model_path)\n",
    "        \n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        self.pose_dirs = {\n",
    "            \"standing\": os.path.join(output_dir, \"standing\"),\n",
    "            \"walking\": os.path.join(output_dir, \"walking\"),\n",
    "            \"sitting\": os.path.join(output_dir, \"sitting\"),\n",
    "            \"pawing\": os.path.join(output_dir, \"pawing\"),\n",
    "        }\n",
    "        for dir_path in self.pose_dirs.values():\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        self.prev_positions = None\n",
    "        self.movement_buffer = []\n",
    "        self.state_buffer = []\n",
    "        self.visualizer = CombinedVisualizer()\n",
    "        \n",
    "        # New variables for improved pawing detection\n",
    "        self.front_paw_positions_history = deque(maxlen=30)  # Store past positions for pattern analysis\n",
    "        self.front_leg_angles_history = deque(maxlen=30)     # Store past angles for pattern analysis\n",
    "        self.pawing_pattern_count = 0                       # Counter for potential pawing patterns\n",
    "        self.pawing_cooldown = 0                           # Cooldown after pawing is detected\n",
    "        self.pawing_detection_threshold = 3                # Number of repetitions to confirm pawing\n",
    "        self.pawing_angle_threshold = (100, 140)           # Angle range for potential pawing\n",
    "\n",
    "    def calculate_angle(self, p1, p2, p3):\n",
    "        \"\"\"Calculate angle between three points (CPU-based for simplicity).\"\"\"\n",
    "        v1 = p1 - p2\n",
    "        v2 = p3 - p2\n",
    "        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        angle = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))\n",
    "        return angle\n",
    "\n",
    "    def detect_pawing_pattern(self, front_paw_positions, front_leg_angles):\n",
    "        \"\"\"\n",
    "        Detect pawing pattern by analyzing the history of front paw movements and leg angles.\n",
    "        Pawing is characterized by repetitive vertical movement of front legs with specific angle patterns.\n",
    "        \"\"\"\n",
    "        # Add current positions and angles to history\n",
    "        self.front_paw_positions_history.append(front_paw_positions)\n",
    "        self.front_leg_angles_history.append(front_leg_angles)\n",
    "        \n",
    "        # Need enough history to detect pattern\n",
    "        if len(self.front_paw_positions_history) < 10:\n",
    "            return False\n",
    "            \n",
    "        # Check for pawing in either front leg\n",
    "        for leg_idx in range(2):  # 0 for left front, 1 for right front\n",
    "            # Extract vertical (y) position history for this leg\n",
    "            y_positions = np.array([pos[leg_idx][1] for pos in self.front_paw_positions_history])\n",
    "            \n",
    "            # Calculate vertical movements (positive values = downward movement)\n",
    "            y_movements = np.diff(y_positions)\n",
    "            \n",
    "            # Check angle history for this leg\n",
    "            angles = np.array([angles[leg_idx] for angles in self.front_leg_angles_history])\n",
    "            \n",
    "            # Pawing pattern detection:\n",
    "            # 1. Find direction changes (up to down and down to up)\n",
    "            direction_changes = np.sign(y_movements[1:]) != np.sign(y_movements[:-1])\n",
    "            num_direction_changes = np.sum(direction_changes)\n",
    "            \n",
    "            # 2. Check if we have at least 3 direction changes (up-down-up or down-up-down)\n",
    "            if num_direction_changes >= 3:\n",
    "                # 3. Check vertical range of movement (should be significant for pawing)\n",
    "                vertical_range = np.max(y_positions) - np.min(y_positions)\n",
    "                \n",
    "                # 4. Check if angles were in pawing range at some point\n",
    "                angles_in_range = np.any((angles >= self.pawing_angle_threshold[0]) & \n",
    "                                         (angles <= self.pawing_angle_threshold[1]))\n",
    "                \n",
    "                # Confirm pawing if movement range is significant and angles were appropriate\n",
    "                if vertical_range > 15 and angles_in_range:\n",
    "                    return True\n",
    "                    \n",
    "        return False\n",
    "\n",
    "    def detect_state(self, keypoints):\n",
    "        \"\"\"Enhanced detect_state method with improved pawing detection.\"\"\"\n",
    "        movement_detected = False\n",
    "\n",
    "        for person_id, kp_array in keypoints.items():\n",
    "            leg_points = {\n",
    "                \"L_F_Hip\": kp_array[5][:2],\n",
    "                \"L_F_Knee\": kp_array[6][:2],\n",
    "                \"L_F_Paw\": kp_array[7][:2],\n",
    "                \"R_F_Hip\": kp_array[8][:2],\n",
    "                \"R_F_Knee\": kp_array[9][:2],\n",
    "                \"R_F_Paw\": kp_array[10][:2],\n",
    "                \"L_B_Hip\": kp_array[11][:2],\n",
    "                \"L_B_Knee\": kp_array[12][:2],\n",
    "                \"L_B_Paw\": kp_array[13][:2],\n",
    "                \"R_B_Hip\": kp_array[14][:2],\n",
    "                \"R_B_Knee\": kp_array[15][:2],\n",
    "                \"R_B_Paw\": kp_array[16][:2]\n",
    "            }\n",
    "\n",
    "            angles = {\n",
    "                \"left_front\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"L_F_Hip\"]),\n",
    "                    np.array(leg_points[\"L_F_Knee\"]),\n",
    "                    np.array(leg_points[\"L_F_Paw\"])\n",
    "                ),\n",
    "                \"right_front\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"R_F_Hip\"]),\n",
    "                    np.array(leg_points[\"R_F_Knee\"]),\n",
    "                    np.array(leg_points[\"R_F_Paw\"])\n",
    "                ),\n",
    "                \"left_back\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"L_B_Hip\"]),\n",
    "                    np.array(leg_points[\"L_B_Knee\"]),\n",
    "                    np.array(leg_points[\"L_B_Paw\"])\n",
    "                ),\n",
    "                \"right_back\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"R_B_Hip\"]),\n",
    "                    np.array(leg_points[\"R_B_Knee\"]),\n",
    "                    np.array(leg_points[\"R_B_Paw\"])\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            # Extract front paw positions for pattern analysis\n",
    "            front_paw_positions = [\n",
    "                leg_points[\"L_F_Paw\"],\n",
    "                leg_points[\"R_F_Paw\"]\n",
    "            ]\n",
    "            \n",
    "            # Extract front leg angles for pattern analysis\n",
    "            front_leg_angles = [\n",
    "                angles[\"left_front\"],\n",
    "                angles[\"right_front\"]\n",
    "            ]\n",
    "            \n",
    "            # Improved pawing detection using repetitive pattern analysis\n",
    "            is_pawing_pattern = self.detect_pawing_pattern(front_paw_positions, front_leg_angles)\n",
    "            \n",
    "            # State determination with cooldown logic for pawing\n",
    "            if self.pawing_cooldown > 0:\n",
    "                self.pawing_cooldown -= 1\n",
    "                return \"pawing\"\n",
    "                \n",
    "            if is_pawing_pattern:\n",
    "                self.pawing_pattern_count += 1\n",
    "                if self.pawing_pattern_count >= self.pawing_detection_threshold:\n",
    "                    self.pawing_pattern_count = 0\n",
    "                    self.pawing_cooldown = 15  # Keep \"pawing\" state for 15 frames after detection\n",
    "                    return \"pawing\"\n",
    "            else:\n",
    "                # Gradually decrease the pattern count when no pawing is detected\n",
    "                self.pawing_pattern_count = max(0, self.pawing_pattern_count - 0.5)\n",
    "            \n",
    "            # Check for sitting behavior\n",
    "            if all(angle < 90 for angle in angles.values()):\n",
    "                return \"sitting\"\n",
    "\n",
    "            # Movement detection for walking vs standing\n",
    "            paw_positions = np.array([\n",
    "                leg_points[\"L_F_Paw\"],\n",
    "                leg_points[\"R_F_Paw\"],\n",
    "                leg_points[\"L_B_Paw\"],\n",
    "                leg_points[\"R_B_Paw\"]\n",
    "            ])\n",
    "\n",
    "            if self.prev_positions is not None:\n",
    "                movements = np.linalg.norm(paw_positions - self.prev_positions, axis=1)\n",
    "                self.movement_buffer.append(np.mean(movements))\n",
    "                if len(self.movement_buffer) > 10:\n",
    "                    self.movement_buffer.pop(0)\n",
    "                movement_detected = np.mean(self.movement_buffer) > 5.0\n",
    "\n",
    "            self.prev_positions = paw_positions\n",
    "\n",
    "        current_state = \"walking\" if movement_detected else \"standing\"\n",
    "        self.state_buffer.append(current_state)\n",
    "        if len(self.state_buffer) > 15:\n",
    "            self.state_buffer.pop(0)\n",
    "        return max(set(self.state_buffer), key=self.state_buffer.count)\n",
    "\n",
    "    def draw_state_annotation(self, frame, state):\n",
    "        \"\"\"Draw state annotation on frame without detailed visualization.\"\"\"\n",
    "        # Simply return the original frame without any annotations\n",
    "        return frame\n",
    "\n",
    "    def save_frame(self, frame, state, frame_count):\n",
    "        \"\"\"Save frame to appropriate directory based on state.\"\"\"\n",
    "        if state in self.pose_dirs:\n",
    "            filename = os.path.join(self.pose_dirs[state], f\"frame_{frame_count}.jpg\")\n",
    "            cv2.imwrite(filename, frame)\n",
    "\n",
    "    def draw_keypoints(self, frame, keypoints):\n",
    "        \"\"\"Return original frame without keypoints drawing.\"\"\"\n",
    "        # Simply return the original frame without drawing keypoints\n",
    "        return frame\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        \"\"\"Process video file and analyze horse gait on GPU.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        combined_height = height + 150  # Add space for visualizer\n",
    "        output_path = os.path.join(self.output_dir, \"video_with_analysis2.mp4\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, combined_height))\n",
    "\n",
    "        frame_count = 0\n",
    "        last_announced_state = None\n",
    "\n",
    "        cv2.namedWindow(\"Horse Gait Analysis\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                keypoints = self.model.infer(frame)\n",
    "                current_state = self.detect_state(keypoints)\n",
    "                \n",
    "                # No keypoint visualization\n",
    "                annotated_frame = frame.copy()\n",
    "                \n",
    "                self.save_frame(annotated_frame, current_state, frame_count)\n",
    "\n",
    "                self.visualizer.update(current_state)\n",
    "                combined_display = self.visualizer.create_visualization(annotated_frame)\n",
    "\n",
    "                cv2.imshow(\"Horse Gait Analysis\", combined_display)\n",
    "                out.write(combined_display)\n",
    "\n",
    "                frame_count += 1\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "\n",
    "        finally:\n",
    "            cap.release()\n",
    "            out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(f\"\\nProcessing complete! Output saved to: {output_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the horse gait analysis on GPU.\"\"\"\n",
    "    engine_path = \"C:/Users/hp/Downloads/vitpose-l-ap10k.engine\"\n",
    "    try:\n",
    "        # Print GPU info\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        monitor = HorseGaitMonitor(engine_path)\n",
    "        monitor.process_video(r\"E:\\vitpose\\strach.mp4\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd0d991-2794-4d30-bf7c-fcd8232c5a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9841c-490a-4d4c-b356-81b2f87152be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44989a-f3e9-4dce-a16a-f4a9584a2147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5eec5-823c-48c2-9880-9e1c44778d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8b45646-ab6f-4f05-bd2f-d518d11eb360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting C:/Users/hp/Downloads/vitpose-l-ap10k.onnx to TensorRT engine (FP32)...\n",
      "Tensorrt input \"input_0\" with shape(-1, 3, 256, 192) DataType.FLOAT\n",
      "Tensorrt output \"output_0\" with shape(-1, 17, 64, 48) DataType.FLOAT\n",
      "Tensorrt building FP32 engine\n",
      "Successfully converted to TensorRT engine (FP32): C:/Users/hp/Downloads/vitpose-l-ap10k.engine\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorrt as trt\n",
    "import os\n",
    "\n",
    "# Model paths\n",
    "ONNX_PATH = \"C:/Users/hp/Downloads/vitpose-l-ap10k.onnx\"\n",
    "TRT_PATH = ONNX_PATH.replace('.onnx', '.engine')\n",
    "\n",
    "# Model configuration\n",
    "C, H, W = (3, 256, 192)\n",
    "input_names = [\"input_0\"]\n",
    "output_names = [\"output_0\"]\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create dummy input tensor\n",
    "inputs = torch.randn(1, C, H, W).to(device)\n",
    "\n",
    "# Dynamic axes configuration\n",
    "dynamic_axes = {\n",
    "    'input_0': {0: 'batch_size'},\n",
    "    'output_0': {0: 'batch_size'}\n",
    "}\n",
    "\n",
    "def export_engine(onnx, im, file, workspace=4, verbose=False, prefix='Tensorrt'):\n",
    "    logger = trt.Logger(trt.Logger.INFO)\n",
    "    if verbose:\n",
    "        logger.min_severity = trt.Logger.Severity.VERBOSE\n",
    "    \n",
    "    # Initialize builder and config\n",
    "    builder = trt.Builder(logger)\n",
    "    config = builder.create_builder_config()\n",
    "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace << 30)\n",
    "    \n",
    "    # Create network\n",
    "    flag = (1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    network = builder.create_network(flag)\n",
    "    \n",
    "    # Parse ONNX\n",
    "    parser = trt.OnnxParser(network, logger)\n",
    "    if not parser.parse_from_file(str(onnx)):\n",
    "        raise RuntimeError(f'failed to load ONNX file: {onnx}')\n",
    "\n",
    "    # Process inputs and outputs\n",
    "    inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
    "    outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
    "    \n",
    "    # Print input and output details\n",
    "    for inp in inputs:\n",
    "        print(f'{prefix} input \"{inp.name}\" with shape{inp.shape} {inp.dtype}')\n",
    "    for out in outputs:\n",
    "        print(f'{prefix} output \"{out.name}\" with shape{out.shape} {out.dtype}')\n",
    "\n",
    "    # Set profile for dynamic shapes\n",
    "    profile = builder.create_optimization_profile()\n",
    "    for inp in inputs:\n",
    "        profile.set_shape(\n",
    "            inp.name, \n",
    "            (1, *im.shape[1:]),                           # min shape\n",
    "            (max(1, im.shape[0] // 2), *im.shape[1:]),    # optimal shape\n",
    "            im.shape                                       # max shape\n",
    "        )\n",
    "    config.add_optimization_profile(profile)\n",
    "\n",
    "    # Force FP32 precision - no FP16 configuration\n",
    "    print(f'{prefix} building FP32 engine')\n",
    "    \n",
    "    # Build and save engine\n",
    "    try:\n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "    except AttributeError:\n",
    "        plan = builder.build_engine(network, config)\n",
    "        serialized_engine = plan.serialize()\n",
    "        plan.destroy()\n",
    "    \n",
    "    if serialized_engine is None:\n",
    "        raise RuntimeError(f'{prefix} failed to build TensorRT engine')\n",
    "    \n",
    "    with open(file, 'wb') as f:\n",
    "        f.write(serialized_engine)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Check if ONNX file exists\n",
    "if not os.path.exists(ONNX_PATH):\n",
    "    raise FileNotFoundError(f\"ONNX file not found at {ONNX_PATH}\")\n",
    "\n",
    "# Convert to TensorRT\n",
    "print(f\"Converting {ONNX_PATH} to TensorRT engine (FP32)...\")\n",
    "success = export_engine(\n",
    "    onnx=ONNX_PATH,\n",
    "    im=inputs,\n",
    "    file=TRT_PATH,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(f\"Successfully converted to TensorRT engine (FP32): {TRT_PATH}\")\n",
    "else:\n",
    "    print(\"Conversion failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f3d3940-8568-474e-916c-64f674664804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA Version: 12.1\n",
      "Starting model validation with image...\n",
      "\n",
      "Running ONNX inference...\n",
      "ONNX output shape: torch.Size([1, 17, 64, 48])\n",
      "\n",
      "Running TensorRT inference...\n",
      "TensorRT output shape: torch.Size([1, 17, 64, 48])\n",
      "\n",
      "Validation Results:\n",
      "Outputs match within tolerance: True\n",
      "Maximum absolute difference: 0.000544\n",
      "Mean absolute difference: 0.000002\n",
      "\n",
      "ONNX Keypoints (x, y, confidence):\n",
      "Keypoint 0: (285, 102), Confidence: 0.964143\n",
      "Keypoint 1: (224, 91), Confidence: 0.919919\n",
      "Keypoint 2: (204, 234), Confidence: 0.887964\n",
      "Keypoint 3: (469, 193), Confidence: 0.698245\n",
      "Keypoint 4: (735, 91), Confidence: 0.838982\n",
      "Keypoint 5: (530, 346), Confidence: 0.960392\n",
      "Keypoint 6: (490, 469), Confidence: 0.938401\n",
      "Keypoint 7: (449, 591), Confidence: 0.910693\n",
      "Keypoint 8: (428, 336), Confidence: 0.885177\n",
      "Keypoint 9: (408, 459), Confidence: 0.951775\n",
      "Keypoint 10: (367, 581), Confidence: 0.921609\n",
      "Keypoint 11: (775, 265), Confidence: 0.904391\n",
      "Keypoint 12: (775, 367), Confidence: 0.944347\n",
      "Keypoint 13: (755, 479), Confidence: 1.009821\n",
      "Keypoint 14: (694, 285), Confidence: 0.884687\n",
      "Keypoint 15: (694, 367), Confidence: 0.835837\n",
      "Keypoint 16: (673, 479), Confidence: 0.959095\n",
      "\n",
      "TensorRT Keypoints (x, y, confidence):\n",
      "Keypoint 0: (285, 102), Confidence: 0.964080\n",
      "Keypoint 1: (224, 91), Confidence: 0.919975\n",
      "Keypoint 2: (204, 234), Confidence: 0.887875\n",
      "Keypoint 3: (469, 193), Confidence: 0.698260\n",
      "Keypoint 4: (735, 91), Confidence: 0.839225\n",
      "Keypoint 5: (530, 346), Confidence: 0.960386\n",
      "Keypoint 6: (490, 469), Confidence: 0.938422\n",
      "Keypoint 7: (449, 591), Confidence: 0.910660\n",
      "Keypoint 8: (428, 336), Confidence: 0.885250\n",
      "Keypoint 9: (408, 459), Confidence: 0.952066\n",
      "Keypoint 10: (367, 581), Confidence: 0.921583\n",
      "Keypoint 11: (775, 265), Confidence: 0.904287\n",
      "Keypoint 12: (775, 367), Confidence: 0.944151\n",
      "Keypoint 13: (755, 479), Confidence: 1.010212\n",
      "Keypoint 14: (694, 285), Confidence: 0.884550\n",
      "Keypoint 15: (694, 357), Confidence: 0.835837\n",
      "Keypoint 16: (673, 479), Confidence: 0.958913\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import cv2\n",
    "\n",
    "class TensorRTValidator:\n",
    "    def __init__(self, engine_path):\n",
    "        self.logger = trt.Logger(trt.Logger.INFO)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "        \n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_bytes = f.read()\n",
    "            self.engine = self.runtime.deserialize_cuda_engine(engine_bytes)\n",
    "            \n",
    "        if not self.engine:\n",
    "            raise RuntimeError(f\"Failed to load TensorRT engine from {engine_path}\")\n",
    "            \n",
    "        self.context = self.engine.create_execution_context()\n",
    "        self.allocate_buffers()\n",
    "\n",
    "    def allocate_buffers(self):\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.bindings = []\n",
    "\n",
    "        num_io_tensors = self.engine.num_io_tensors\n",
    "        for i in range(num_io_tensors):\n",
    "            tensor_name = self.engine.get_tensor_name(i)\n",
    "            tensor_shape = self.engine.get_tensor_shape(tensor_name)\n",
    "            tensor_dtype = trt.nptype(self.engine.get_tensor_dtype(tensor_name))\n",
    "            \n",
    "            if tensor_dtype == np.float32:\n",
    "                torch_dtype = torch.float32\n",
    "            elif tensor_dtype == np.float16:\n",
    "                torch_dtype = torch.float16\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported dtype: {tensor_dtype}\")\n",
    "\n",
    "            tensor = torch.zeros(tuple(tensor_shape), dtype=torch_dtype, device='cuda')\n",
    "            self.bindings.append(tensor.data_ptr())\n",
    "            \n",
    "            if self.engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:\n",
    "                self.inputs.append(tensor)\n",
    "            else:\n",
    "                self.outputs.append(tensor)\n",
    "\n",
    "            self.context.set_tensor_address(tensor_name, tensor.data_ptr())\n",
    "\n",
    "    def infer(self, input_tensor):\n",
    "        self.inputs[0].copy_(input_tensor)\n",
    "        self.context.execute_async_v3(stream_handle=torch.cuda.current_stream().cuda_stream)\n",
    "        torch.cuda.synchronize()\n",
    "        return self.outputs[0].clone()\n",
    "\n",
    "def preprocess_image(image_path, target_size=(256, 192)):\n",
    "    \"\"\"Load and preprocess image for model input.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image from {image_path}\")\n",
    "    \n",
    "    orig_img = img.copy()\n",
    "    img = cv2.resize(img, (target_size[1], target_size[0]))  # (width, height)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32) / 255.0  # Ensure float32 here\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    img = (img - mean) / std\n",
    "    # Convert to tensor and ensure float32\n",
    "    img_tensor = torch.from_numpy(img.transpose(2, 0, 1)).unsqueeze(0).to(torch.float32).cuda()\n",
    "    return img_tensor, orig_img\n",
    "\n",
    "def extract_keypoints(heatmap, threshold=0.1):\n",
    "    \"\"\"Extract keypoints from heatmap tensor.\"\"\"\n",
    "    batch_size, num_keypoints, height, width = heatmap.shape\n",
    "    keypoints = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        batch_keypoints = []\n",
    "        for k in range(num_keypoints):\n",
    "            heatmap_k = heatmap[b, k]\n",
    "            max_val = torch.max(heatmap_k)\n",
    "            if max_val < threshold:\n",
    "                batch_keypoints.append((-1, -1, float(max_val)))\n",
    "                continue\n",
    "            y, x = torch.where(heatmap_k == max_val)\n",
    "            batch_keypoints.append((int(x[0]), int(y[0]), float(max_val)))\n",
    "        keypoints.append(batch_keypoints)\n",
    "    return keypoints\n",
    "\n",
    "def draw_keypoints(image, keypoints, color=(0, 255, 0)):\n",
    "    \"\"\"Draw keypoints on the image.\"\"\"\n",
    "    for x, y, conf in keypoints:\n",
    "        if x != -1 and y != -1:\n",
    "            cv2.circle(image, (x, y), 5, color, -1)\n",
    "    return image\n",
    "\n",
    "def validate_models_with_image(onnx_path, engine_path, image_path):\n",
    "    print(\"Starting model validation with image...\")\n",
    "    \n",
    "    input_tensor, orig_img = preprocess_image(image_path)\n",
    "    \n",
    "    # ONNX inference\n",
    "    print(\"\\nRunning ONNX inference...\")\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "    ort_session = onnxruntime.InferenceSession(onnx_path, providers=providers)\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: input_tensor.cpu().numpy()}  # float32 ensured\n",
    "    onnx_output_np = ort_session.run(None, ort_inputs)[0]\n",
    "    onnx_output = torch.from_numpy(onnx_output_np).cuda()\n",
    "    print(\"ONNX output shape:\", onnx_output.shape)\n",
    "    \n",
    "    # TensorRT inference\n",
    "    print(\"\\nRunning TensorRT inference...\")\n",
    "    trt_validator = TensorRTValidator(engine_path)\n",
    "    trt_output = trt_validator.infer(input_tensor)\n",
    "    print(\"TensorRT output shape:\", trt_output.shape)\n",
    "    \n",
    "    # Compare outputs\n",
    "    diff = torch.abs(onnx_output - trt_output)\n",
    "    max_diff = float(torch.max(diff))\n",
    "    mean_diff = float(torch.mean(diff))\n",
    "    is_close = torch.allclose(onnx_output, trt_output, rtol=1e-3, atol=1e-3)\n",
    "    \n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Outputs match within tolerance: {is_close}\")\n",
    "    print(f\"Maximum absolute difference: {max_diff:.6f}\")\n",
    "    print(f\"Mean absolute difference: {mean_diff:.6f}\")\n",
    "    \n",
    "    # Extract keypoints\n",
    "    onnx_keypoints = extract_keypoints(onnx_output)[0]\n",
    "    trt_keypoints = extract_keypoints(trt_output)[0]\n",
    "    \n",
    "    # Scale keypoints to original image size\n",
    "    orig_h, orig_w = orig_img.shape[:2]\n",
    "    scale_x = orig_w / 48\n",
    "    scale_y = orig_h / 64\n",
    "    onnx_keypoints_scaled = [(int(x * scale_x), int(y * scale_y), conf) if x != -1 else (-1, -1, conf) for x, y, conf in onnx_keypoints]\n",
    "    trt_keypoints_scaled = [(int(x * scale_x), int(y * scale_y), conf) if x != -1 else (-1, -1, conf) for x, y, conf in trt_keypoints]\n",
    "    \n",
    "    # Print keypoints\n",
    "    print(\"\\nONNX Keypoints (x, y, confidence):\")\n",
    "    for i, (x, y, conf) in enumerate(onnx_keypoints_scaled):\n",
    "        print(f\"Keypoint {i}: ({x}, {y}), Confidence: {conf:.6f}\")\n",
    "    \n",
    "    print(\"\\nTensorRT Keypoints (x, y, confidence):\")\n",
    "    for i, (x, y, conf) in enumerate(trt_keypoints_scaled):\n",
    "        print(f\"Keypoint {i}: ({x}, {y}), Confidence: {conf:.6f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    onnx_img = draw_keypoints(orig_img.copy(), onnx_keypoints_scaled, color=(0, 255, 0))\n",
    "    trt_img = draw_keypoints(orig_img.copy(), trt_keypoints_scaled, color=(255, 0, 0))\n",
    "    \n",
    "    cv2.imshow(\"ONNX Keypoints\", onnx_img)\n",
    "    cv2.imshow(\"TensorRT Keypoints\", trt_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. Please check your PyTorch installation.\")\n",
    "        \n",
    "    ONNX_PATH = \"C:/Users/hp/Downloads/vitpose-l-ap10k.onnx\"\n",
    "    TRT_PATH = ONNX_PATH.replace('.onnx', '.engine')\n",
    "    IMAGE_PATH = \"E:/lame.webp\"  # Replace with your image path\n",
    "    \n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    validate_models_with_image(ONNX_PATH, TRT_PATH, IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde90a2-1f69-4ef3-8f24-969ba676a980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c3ddc-f706-4c0d-862d-59c24c67fb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504c20a-bc12-4eb2-8a34-a17324fe00e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf167014-82f9-4f82-9207-27d99030f507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.8.0.43\n"
     ]
    }
   ],
   "source": [
    "print(trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a8a0a7-589e-4caa-91f0-ec84f63efa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from easy_ViTPose.easy_ViTPose.vit_models.model import ViTPose\n",
    "from easy_ViTPose.easy_ViTPose.configs.ViTPose_coco import model_small as model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "377bc5bb-aa4a-4104-9e8d-42796bab8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_engine(onnx, im, file, half, dynamic, workspace=4, verbose=False, prefix='Tensorrt'):\n",
    "    logger = trt.Logger(trt.Logger.INFO)\n",
    "    if verbose:\n",
    "        logger.min_severity = trt.Logger.Severity.VERBOSE\n",
    "    \n",
    "    # Initialize builder and config\n",
    "    builder = trt.Builder(logger)\n",
    "    config = builder.create_builder_config()\n",
    "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace << 30)\n",
    "    \n",
    "    # Create network\n",
    "    flag = (1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    network = builder.create_network(flag)\n",
    "    \n",
    "    # Parse ONNX\n",
    "    parser = trt.OnnxParser(network, logger)\n",
    "    if not parser.parse_from_file(str(onnx)):\n",
    "        raise RuntimeError(f'failed to load ONNX file: {onnx}')\n",
    "\n",
    "    # Process inputs and outputs\n",
    "    inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
    "    outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
    "    \n",
    "    # Print input and output details\n",
    "    for inp in inputs:\n",
    "        print(f'{prefix} input \"{inp.name}\" with shape{inp.shape} {inp.dtype}')\n",
    "    for out in outputs:\n",
    "        print(f'{prefix} output \"{out.name}\" with shape{out.shape} {out.dtype}')\n",
    "\n",
    "    # Handle dynamic batching\n",
    "    if dynamic:\n",
    "        if im.shape[0] <= 1:\n",
    "            print(f'{prefix} WARNING  --dynamic model requires maximum --batch-size argument')\n",
    "        profile = builder.create_optimization_profile()\n",
    "        for inp in inputs:\n",
    "            profile.set_shape(\n",
    "                inp.name, \n",
    "                (1, *im.shape[1:]),                           # min shape\n",
    "                (max(1, im.shape[0] // 2), *im.shape[1:]),    # optimal shape\n",
    "                im.shape                                       # max shape\n",
    "            )\n",
    "        config.add_optimization_profile(profile)\n",
    "\n",
    "    # Configure precision\n",
    "    print(f'{prefix} building FP{16 if builder.platform_has_fast_fp16 and half else 32} engine')\n",
    "    if builder.platform_has_fast_fp16 and half:\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "    # Build and save engine\n",
    "    try:\n",
    "        # First try the new API (TensorRT 8.4+)\n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "    except AttributeError:\n",
    "        # Fallback for older versions\n",
    "        plan = builder.build_engine(network, config)\n",
    "        serialized_engine = plan.serialize()\n",
    "        plan.destroy()\n",
    "    \n",
    "    if serialized_engine is None:\n",
    "        raise RuntimeError(f'{prefix} failed to build TensorRT engine')\n",
    "    \n",
    "    with open(file, 'wb') as f:\n",
    "        f.write(serialized_engine)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e5ad935-3634-447a-8d9f-fb9837d68fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ONNX_PATH = \"C:/Users/hp/Downloads/vitpose-l-ap10k.onnx\"\n",
    "TRT_PATH = ONNX_PATH.replace('.onnx', '.engine')\n",
    "C, H, W = (3, 256, 192)\n",
    "\n",
    "input_names = [\"input_0\"]\n",
    "output_names = [\"output_0\"]\n",
    "\n",
    "# Use CUDA if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a dummy input tensor\n",
    "inputs = torch.randn(1, C, H, W).to(device)\n",
    "\n",
    "dynamic_axes = {'input_0' : {0 : 'batch_size'},\n",
    "                'output_0' : {0 : 'batch_size'}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "febb3083-ff31-4dad-a123-faf8a36d813c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorrt input \"input_0\" with shape(-1, 3, 256, 192) DataType.FLOAT\n",
      "Tensorrt output \"output_0\" with shape(-1, 17, 64, 48) DataType.FLOAT\n",
      "Tensorrt WARNING  --dynamic model requires maximum --batch-size argument\n",
      "Tensorrt building FP32 engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_engine(ONNX_PATH, inputs, TRT_PATH, False, True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840f6af-9187-4b94-a5a4-8a52ed94fd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb35d2-c383-4d4d-96ff-b029ef9f7961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0460995-6d8b-41cb-a4c9-efdf9f0c5aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46b7a5d6-9ca7-4a45-a1b7-696520e011b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 0: input_0, Shape: (1, 3, 256, 192), Mode: TensorIOMode.INPUT, Dtype: DataType.FLOAT\n",
      "Tensor 1: output_0, Shape: (1, 17, 64, 48), Mode: TensorIOMode.OUTPUT, Dtype: DataType.FLOAT\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "logger = trt.Logger(trt.Logger.INFO)\n",
    "runtime = trt.Runtime(logger)\n",
    "with open(\"C:/Users/hp/Downloads/vitpose-l-ap10k.engine\", 'rb') as f:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "for i in range(engine.num_io_tensors):\n",
    "    name = engine.get_tensor_name(i)\n",
    "    shape = engine.get_tensor_shape(name)\n",
    "    mode = engine.get_tensor_mode(name)\n",
    "    dtype = engine.get_tensor_dtype(name)\n",
    "    print(f\"Tensor {i}: {name}, Shape: {shape}, Mode: {mode}, Dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a7249-dc84-4e65-ba0b-fe33cd584c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe4184-1363-48ef-b786-ba46e5155c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4a3d48f-f4e5-4e5d-b76b-53d4a137d76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0: Horse is now standing\n",
      "Frame 1: Horse is now walking\n",
      "Frame 5: Horse is now pawing\n",
      "Frame 6: Horse is now walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_14420\\391320919.py:209: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 10: Horse is now pawing\n",
      "Frame 11: Horse is now walking\n",
      "Frame 12: Horse is now pawing\n",
      "Frame 13: Horse is now walking\n",
      "Frame 14: Horse is now pawing\n",
      "Frame 15: Horse is now walking\n",
      "Frame 22: Horse is now pawing\n",
      "Frame 23: Horse is now walking\n",
      "Frame 28: Horse is now pawing\n",
      "Frame 29: Horse is now walking\n",
      "Processing progress: 0.0%\n",
      "Frame 30: Horse is now sitting\n",
      "Frame 32: Horse is now walking\n",
      "Frame 33: Horse is now pawing\n",
      "Frame 34: Horse is now walking\n",
      "Frame 39: Horse is now pawing\n",
      "Frame 41: Horse is now walking\n",
      "Frame 43: Horse is now sitting\n",
      "Frame 44: Horse is now walking\n",
      "Processing progress: 0.0%\n",
      "Frame 60: Horse is now pawing\n",
      "Frame 61: Horse is now walking\n",
      "Frame 63: Horse is now pawing\n",
      "Frame 64: Horse is now walking\n",
      "Processing progress: 0.0%\n",
      "\n",
      "Processing complete! Output saved to: monitoring_output\\video_with_analysis.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 377\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 377\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 371\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     monitor \u001b[38;5;241m=\u001b[39m HorseGaitMonitor(engine_path)\n\u001b[1;32m--> 371\u001b[0m     \u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE:/walk.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing video: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 336\u001b[0m, in \u001b[0;36mHorseGaitMonitor.process_video\u001b[1;34m(self, video_path)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m current_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetect_state(keypoints)\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_state \u001b[38;5;241m!=\u001b[39m last_announced_state:\n",
      "Cell \u001b[1;32mIn[19], line 177\u001b[0m, in \u001b[0;36mTensorRTInference.infer\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy_(torch_input)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mexecute_async_v3(stream_handle\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_stream()\u001b[38;5;241m.\u001b[39mcuda_stream)\n\u001b[1;32m--> 177\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    180\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess_output(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:954\u001b[0m, in \u001b[0;36msynchronize\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    952\u001b[0m _lazy_init()\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "class CombinedVisualizer:\n",
    "    def __init__(self, window_size=100):\n",
    "        self.window_size = window_size\n",
    "        self.activities = deque(maxlen=window_size)\n",
    "        self.activity_counts = {\n",
    "            \"walking\": 0,\n",
    "            \"standing\": 0,\n",
    "            \"sitting\": 0,\n",
    "            \"pawing\": 0,\n",
    "            \"Unknown\": 0,\n",
    "        }\n",
    "        self.total_frames = 0\n",
    "\n",
    "    def update(self, activity):\n",
    "        self.activities.append(activity)\n",
    "        self.activity_counts[activity] += 1\n",
    "        self.total_frames += 1\n",
    "\n",
    "    def create_visualization(self, frame):\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        defrag_height = 150\n",
    "        defrag_width = frame_width\n",
    "        defrag_image = np.ones((defrag_height, defrag_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        colors = {\n",
    "            \"walking\": (0, 0, 255),  # Red\n",
    "            \"standing\": (255, 0, 0),  # Blue\n",
    "            \"sitting\": (0, 255, 0),  # Green\n",
    "            \"pawing\": (255, 165, 0),  # Orange\n",
    "            \"Unknown\": (128, 128, 128),  # Gray\n",
    "        }\n",
    "\n",
    "        segment_width = defrag_width // self.window_size\n",
    "        for i, activity in enumerate(self.activities):\n",
    "            x_start = i * segment_width\n",
    "            x_end = x_start + segment_width\n",
    "            color = colors.get(activity, (128, 128, 128))\n",
    "            cv2.rectangle(defrag_image, (x_start, 0), (x_end, 80), color, -1)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        legend_items = [\n",
    "            (\"Walking\", (0, 0, 255)),\n",
    "            (\"Standing\", (255, 0, 0)),\n",
    "            (\"Sitting\", (0, 255, 0)),\n",
    "            (\"Pawing\", (255, 165, 0)),\n",
    "            (\"Unknown\", (128, 128, 128)),\n",
    "        ]\n",
    "\n",
    "        x_offset = 10\n",
    "        y_offset = 120\n",
    "        for text, color in legend_items:\n",
    "            cv2.rectangle(\n",
    "                defrag_image,\n",
    "                (x_offset, y_offset - 15),\n",
    "                (x_offset + 20, y_offset + 5),\n",
    "                color,\n",
    "                -1,\n",
    "            )\n",
    "            percentage = (\n",
    "                self.activity_counts.get(text.lower(), 0) / max(1, self.total_frames)\n",
    "            ) * 100\n",
    "            cv2.putText(\n",
    "                defrag_image,\n",
    "                f\"{text}: {percentage:.1f}%\",\n",
    "                (x_offset + 30, y_offset),\n",
    "                font,\n",
    "                0.5,\n",
    "                (0, 0, 0),\n",
    "                1,\n",
    "            )\n",
    "            x_offset += 160\n",
    "\n",
    "        combined_height = frame_height + defrag_height\n",
    "        combined_image = np.zeros((combined_height, frame_width, 3), dtype=np.uint8)\n",
    "        combined_image[:frame_height] = frame\n",
    "        combined_image[frame_height:] = defrag_image\n",
    "\n",
    "        return combined_image\n",
    "\n",
    "class TensorRTInference:\n",
    "    \"\"\"TensorRT inference class for ViTPose model, updated for TensorRT 10.8.0.43\"\"\"\n",
    "    def __init__(self, engine_path):\n",
    "        \"\"\"Initialize TensorRT engine and allocate buffers.\"\"\"\n",
    "        self.logger = trt.Logger(trt.Logger.INFO)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "\n",
    "        # Load engine from file\n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_bytes = f.read()\n",
    "            self.engine = self.runtime.deserialize_cuda_engine(engine_bytes)\n",
    "\n",
    "        if not self.engine:\n",
    "            raise RuntimeError(f\"Failed to load TensorRT engine from {engine_path}\")\n",
    "\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Define input and output shapes (adjust if your .engine file specifies otherwise)\n",
    "        self.input_shape = (1, 3, 256, 192)  # ViTPose standard input\n",
    "        self.output_shape = (1, 17, 64, 48)  # ViTPose standard output (17 keypoints)\n",
    "        \n",
    "        # Tensor names (assuming defaults; adjust based on your engine)\n",
    "        self.input_name = \"input\"\n",
    "        self.output_name = \"output\"\n",
    "        \n",
    "        # Allocate device buffers\n",
    "        self.allocate_buffers()\n",
    "\n",
    "    def allocate_buffers(self):\n",
    "        \"\"\"Allocate CUDA memory for inputs and outputs using TensorRT 10.x API.\"\"\"\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.bindings = []\n",
    "\n",
    "        num_io_tensors = self.engine.num_io_tensors\n",
    "        for i in range(num_io_tensors):\n",
    "            tensor_name = self.engine.get_tensor_name(i)\n",
    "            tensor_shape = self.engine.get_tensor_shape(tensor_name)\n",
    "            tensor_dtype = trt.nptype(self.engine.get_tensor_dtype(tensor_name))\n",
    "            \n",
    "            if tensor_dtype == np.float32:\n",
    "                torch_dtype = torch.float32\n",
    "            elif tensor_dtype == np.float16:\n",
    "                torch_dtype = torch.float16\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported dtype: {tensor_dtype}\")\n",
    "\n",
    "            tensor = torch.zeros(tuple(tensor_shape), dtype=torch_dtype, device='cuda')\n",
    "            self.bindings.append(tensor.data_ptr())\n",
    "            \n",
    "            if self.engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:\n",
    "                self.inputs.append(tensor)\n",
    "            else:\n",
    "                self.outputs.append(tensor)\n",
    "\n",
    "            self.context.set_tensor_address(tensor_name, tensor.data_ptr())\n",
    "\n",
    "    def preprocess_image(self, img):\n",
    "        \"\"\"Preprocess image for model input.\"\"\"\n",
    "        img = cv2.resize(img, (192, 256))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = img.transpose(2, 0, 1)  # HWC to CHW\n",
    "        img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "        return img\n",
    "\n",
    "    def postprocess_output(self, output):\n",
    "        \"\"\"Convert model output to keypoints.\"\"\"\n",
    "        heatmaps = output.reshape(self.output_shape)\n",
    "        keypoints = {}\n",
    "        for batch_idx in range(heatmaps.shape[0]):\n",
    "            person_keypoints = np.zeros((17, 3))  # 17 keypoints (x, y, confidence)\n",
    "            for kpt_idx in range(17):\n",
    "                heatmap = heatmaps[batch_idx, kpt_idx]\n",
    "                flat_idx = np.argmax(heatmap)\n",
    "                y, x = np.unravel_index(flat_idx, heatmap.shape)\n",
    "                orig_x = x * (192 / 48)  # Scale to input width\n",
    "                orig_y = y * (256 / 64)  # Scale to input height\n",
    "                confidence = heatmap[y, x]\n",
    "                person_keypoints[kpt_idx] = [orig_x, orig_y, confidence]\n",
    "            keypoints[batch_idx] = person_keypoints\n",
    "        return keypoints\n",
    "\n",
    "    def infer(self, img):\n",
    "        \"\"\"Run inference on an image.\"\"\"\n",
    "        preprocessed = self.preprocess_image(img)\n",
    "        torch_input = torch.from_numpy(preprocessed).cuda()\n",
    "        self.inputs[0].copy_(torch_input)\n",
    "        \n",
    "        self.context.execute_async_v3(stream_handle=torch.cuda.current_stream().cuda_stream)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        output = self.outputs[0].cpu().numpy()\n",
    "        keypoints = self.postprocess_output(output)\n",
    "        return keypoints\n",
    "\n",
    "class HorseGaitMonitor:\n",
    "    def __init__(self, model_path, output_dir=\"monitoring_output\"):\n",
    "        \"\"\"Initialize the horse gait monitoring system with TensorRT.\"\"\"\n",
    "        self.model = TensorRTInference(model_path)\n",
    "        \n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        self.pose_dirs = {\n",
    "            \"standing\": os.path.join(output_dir, \"standing\"),\n",
    "            \"walking\": os.path.join(output_dir, \"walking\"),\n",
    "            \"sitting\": os.path.join(output_dir, \"sitting\"),\n",
    "            \"pawing\": os.path.join(output_dir, \"pawing\"),\n",
    "        }\n",
    "        for dir_path in self.pose_dirs.values():\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        self.prev_positions = None\n",
    "        self.movement_buffer = []\n",
    "        self.state_buffer = []\n",
    "        self.visualizer = CombinedVisualizer()\n",
    "\n",
    "    def calculate_angle(self, p1, p2, p3):\n",
    "        \"\"\"Calculate angle between three points.\"\"\"\n",
    "        v1 = p1 - p2\n",
    "        v2 = p3 - p2\n",
    "        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        angle = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))\n",
    "        return angle\n",
    "\n",
    "    def detect_state(self, keypoints):\n",
    "        \"\"\"Detect the horse's current state based on keypoints.\"\"\"\n",
    "        movement_detected = False\n",
    "\n",
    "        for person_id, kp_array in keypoints.items():\n",
    "            leg_points = {\n",
    "                \"L_F_Hip\": kp_array[5][:2],\n",
    "                \"L_F_Knee\": kp_array[6][:2],\n",
    "                \"L_F_Paw\": kp_array[7][:2],\n",
    "                \"R_F_Hip\": kp_array[8][:2],\n",
    "                \"R_F_Knee\": kp_array[9][:2],\n",
    "                \"R_F_Paw\": kp_array[10][:2],\n",
    "                \"L_B_Hip\": kp_array[11][:2],\n",
    "                \"L_B_Knee\": kp_array[12][:2],\n",
    "                \"L_B_Paw\": kp_array[13][:2],\n",
    "                \"R_B_Hip\": kp_array[14][:2],\n",
    "                \"R_B_Knee\": kp_array[15][:2],\n",
    "                \"R_B_Paw\": kp_array[16][:2]\n",
    "            }\n",
    "\n",
    "            angles = {\n",
    "                \"left_front\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"L_F_Hip\"]),\n",
    "                    np.array(leg_points[\"L_F_Knee\"]),\n",
    "                    np.array(leg_points[\"L_F_Paw\"])\n",
    "                ),\n",
    "                \"right_front\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"R_F_Hip\"]),\n",
    "                    np.array(leg_points[\"R_F_Knee\"]),\n",
    "                    np.array(leg_points[\"R_F_Paw\"])\n",
    "                ),\n",
    "                \"left_back\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"L_B_Hip\"]),\n",
    "                    np.array(leg_points[\"L_B_Knee\"]),\n",
    "                    np.array(leg_points[\"L_B_Paw\"])\n",
    "                ),\n",
    "                \"right_back\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"R_B_Hip\"]),\n",
    "                    np.array(leg_points[\"R_B_Knee\"]),\n",
    "                    np.array(leg_points[\"R_B_Paw\"])\n",
    "                )\n",
    "            }\n",
    "\n",
    "            if any(100 <= angle <= 130 for angle in [angles[\"left_front\"], angles[\"right_front\"]]):\n",
    "                return \"pawing\"\n",
    "            if all(angle < 90 for angle in angles.values()):\n",
    "                return \"sitting\"\n",
    "\n",
    "            paw_positions = np.array([\n",
    "                leg_points[\"L_F_Paw\"],\n",
    "                leg_points[\"R_F_Paw\"],\n",
    "                leg_points[\"L_B_Paw\"],\n",
    "                leg_points[\"R_B_Paw\"]\n",
    "            ])\n",
    "\n",
    "            if self.prev_positions is not None:\n",
    "                movements = np.linalg.norm(paw_positions - self.prev_positions, axis=1)\n",
    "                self.movement_buffer.append(np.mean(movements))\n",
    "                if len(self.movement_buffer) > 10:\n",
    "                    self.movement_buffer.pop(0)\n",
    "                movement_detected = np.mean(self.movement_buffer) > 5.0\n",
    "\n",
    "            self.prev_positions = paw_positions\n",
    "\n",
    "        current_state = \"walking\" if movement_detected else \"standing\"\n",
    "        self.state_buffer.append(current_state)\n",
    "        if len(self.state_buffer) > 15:\n",
    "            self.state_buffer.pop(0)\n",
    "        return max(set(self.state_buffer), key=self.state_buffer.count)\n",
    "\n",
    "    def draw_state_annotation(self, frame, state):\n",
    "        \"\"\"Draw state annotation on frame.\"\"\"\n",
    "        annotated_frame = frame.copy()\n",
    "        colors = {\n",
    "            \"standing\": (255, 0, 0),  # Blue\n",
    "            \"walking\": (0, 0, 255),   # Red\n",
    "            \"sitting\": (0, 255, 0),   # Green\n",
    "            \"pawing\": (255, 165, 0)   # Orange\n",
    "        }\n",
    "        cv2.putText(\n",
    "            annotated_frame,\n",
    "            f\"State: {state.upper()}\",\n",
    "            (10, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            colors.get(state, (128, 128, 128)),\n",
    "            2\n",
    "        )\n",
    "        return annotated_frame\n",
    "\n",
    "    def save_frame(self, frame, state, frame_count):\n",
    "        \"\"\"Save frame to appropriate directory based on state.\"\"\"\n",
    "        if state in self.pose_dirs:\n",
    "            filename = os.path.join(self.pose_dirs[state], f\"frame_{frame_count}.jpg\")\n",
    "            cv2.imwrite(filename, frame)\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        \"\"\"Process video file and analyze horse gait.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        combined_height = height + 150  # Add space for visualizer\n",
    "        output_path = os.path.join(self.output_dir, \"video_with_analysis.mp4\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, combined_height))\n",
    "\n",
    "        frame_count = 0\n",
    "        last_announced_state = None\n",
    "\n",
    "        cv2.namedWindow(\"Horse Gait Analysis\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                keypoints = self.model.infer(frame)\n",
    "                current_state = self.detect_state(keypoints)\n",
    "\n",
    "                if current_state != last_announced_state:\n",
    "                    print(f\"Frame {frame_count}: Horse is now {current_state}\")\n",
    "                    last_announced_state = current_state\n",
    "\n",
    "                annotated_frame = self.draw_state_annotation(frame, current_state)\n",
    "                self.save_frame(annotated_frame, current_state, frame_count)\n",
    "\n",
    "                self.visualizer.update(current_state)\n",
    "                combined_display = self.visualizer.create_visualization(annotated_frame)\n",
    "\n",
    "                cv2.imshow(\"Horse Gait Analysis\", combined_display)\n",
    "                out.write(combined_display)\n",
    "\n",
    "                frame_count += 1\n",
    "                if frame_count % 30 == 0:\n",
    "                    progress = (frame_count / total_frames) * 100\n",
    "                    print(f\"Processing progress: {progress:.1f}%\")\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "\n",
    "        finally:\n",
    "            cap.release()\n",
    "            out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(f\"\\nProcessing complete! Output saved to: {output_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the horse gait analysis.\"\"\"\n",
    "    engine_path = \"C:/Users/hp/Downloads/vitpose-l-ap10k.engine\"\n",
    "    try:\n",
    "        monitor = HorseGaitMonitor(engine_path)\n",
    "        monitor.process_video(\"E:/walk.mp4\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9878614a-70a8-4bf2-8ca8-651d4525c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581af88a-2b0b-4f3a-984f-188096739241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.8.0.43\n"
     ]
    }
   ],
   "source": [
    "print(trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ac084-d8ad-4e4f-b6d1-4568f2830f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bcf26-e380-4be0-a62e-856af51b25fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c466405-f113-47d8-a8e2-4b457c994752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2153f4d7-d565-4bef-bc86-7cf176d29012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Frame 0: Horse is now standing\n",
      "Frame 12: Horse is now pawing\n",
      "Frame 14: Horse is now standing\n",
      "Frame 27: Horse is now pawing\n",
      "Frame 28: Horse is now standing\n",
      "Processing progress: 3.7%\n",
      "Frame 58: Horse is now pawing\n",
      "Frame 59: Horse is now standing\n",
      "Processing progress: 7.4%\n",
      "Processing progress: 11.2%\n",
      "Processing progress: 14.9%\n",
      "Processing progress: 18.6%\n",
      "Processing progress: 22.3%\n",
      "Processing progress: 26.1%\n",
      "Processing progress: 29.8%\n",
      "Frame 242: Horse is now pawing\n",
      "Frame 243: Horse is now standing\n",
      "Processing progress: 33.5%\n",
      "Processing progress: 37.2%\n",
      "Frame 329: Horse is now pawing\n",
      "Processing progress: 40.9%\n",
      "Frame 330: Horse is now standing\n",
      "Frame 355: Horse is now pawing\n",
      "Frame 357: Horse is now standing\n",
      "Processing progress: 44.7%\n",
      "Frame 368: Horse is now pawing\n",
      "Frame 369: Horse is now standing\n",
      "Frame 382: Horse is now pawing\n",
      "Frame 384: Horse is now standing\n",
      "Processing progress: 48.4%\n",
      "Frame 395: Horse is now pawing\n",
      "Frame 396: Horse is now standing\n",
      "Processing progress: 52.1%\n",
      "Processing progress: 55.8%\n",
      "Processing progress: 59.6%\n",
      "Processing progress: 63.3%\n",
      "Processing progress: 67.0%\n",
      "Processing progress: 70.7%\n",
      "Frame 578: Horse is now pawing\n",
      "Frame 579: Horse is now standing\n",
      "Frame 596: Horse is now pawing\n",
      "Frame 597: Horse is now standing\n",
      "Processing progress: 74.4%\n",
      "Frame 608: Horse is now pawing\n",
      "Frame 612: Horse is now standing\n",
      "Processing progress: 78.2%\n",
      "Frame 640: Horse is now pawing\n",
      "Frame 641: Horse is now standing\n",
      "Processing progress: 81.9%\n",
      "Processing progress: 85.6%\n",
      "Processing progress: 89.3%\n",
      "Processing progress: 93.1%\n",
      "Processing progress: 96.8%\n",
      "\n",
      "Processing complete! Output saved to: monitoring_output\\video_with_analysis2.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# Ensure CUDA is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. Please check your GPU and PyTorch installation.\")\n",
    "\n",
    "class CombinedVisualizer:\n",
    "    def __init__(self, window_size=100):\n",
    "        self.window_size = window_size\n",
    "        self.activities = deque(maxlen=window_size)\n",
    "        self.activity_counts = {\n",
    "            \"walking\": 0,\n",
    "            \"standing\": 0,\n",
    "            \"sitting\": 0,\n",
    "            \"pawing\": 0,\n",
    "            \"Unknown\": 0,\n",
    "        }\n",
    "        self.total_frames = 0\n",
    "\n",
    "    def update(self, activity):\n",
    "        self.activities.append(activity)\n",
    "        self.activity_counts[activity] += 1\n",
    "        self.total_frames += 1\n",
    "\n",
    "    def create_visualization(self, frame):\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        defrag_height = 150\n",
    "        defrag_width = frame_width\n",
    "        defrag_image = np.ones((defrag_height, defrag_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        colors = {\n",
    "            \"walking\": (0, 0, 255),  # Red\n",
    "            \"standing\": (255, 0, 0),  # Blue\n",
    "            \"sitting\": (0, 255, 0),  # Green\n",
    "            \"pawing\": (255, 165, 0),  # Orange\n",
    "            \"Unknown\": (128, 128, 128),  # Gray\n",
    "        }\n",
    "\n",
    "        segment_width = defrag_width // self.window_size\n",
    "        for i, activity in enumerate(self.activities):\n",
    "            x_start = i * segment_width\n",
    "            x_end = x_start + segment_width\n",
    "            color = colors.get(activity, (128, 128, 128))\n",
    "            cv2.rectangle(defrag_image, (x_start, 0), (x_end, 80), color, -1)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        legend_items = [\n",
    "            (\"Walking\", (0, 0, 255)),\n",
    "            (\"Standing\", (255, 0, 0)),\n",
    "            (\"Sitting\", (0, 255, 0)),\n",
    "            (\"Pawing\", (255, 165, 0)),\n",
    "            (\"Unknown\", (128, 128, 128)),\n",
    "        ]\n",
    "\n",
    "        x_offset = 10\n",
    "        y_offset = 120\n",
    "        for text, color in legend_items:\n",
    "            cv2.rectangle(\n",
    "                defrag_image,\n",
    "                (x_offset, y_offset - 15),\n",
    "                (x_offset + 20, y_offset + 5),\n",
    "                color,\n",
    "                -1,\n",
    "            )\n",
    "            percentage = (\n",
    "                self.activity_counts.get(text.lower(), 0) / max(1, self.total_frames)\n",
    "            ) * 100\n",
    "            cv2.putText(\n",
    "                defrag_image,\n",
    "                f\"{text}: {percentage:.1f}%\",\n",
    "                (x_offset + 30, y_offset),\n",
    "                font,\n",
    "                0.5,\n",
    "                (0, 0, 0),\n",
    "                1,\n",
    "            )\n",
    "            x_offset += 160\n",
    "\n",
    "        combined_height = frame_height + defrag_height\n",
    "        combined_image = np.zeros((combined_height, frame_width, 3), dtype=np.uint8)\n",
    "        combined_image[:frame_height] = frame\n",
    "        combined_image[frame_height:] = defrag_image\n",
    "\n",
    "        return combined_image\n",
    "\n",
    "class TensorRTInference:\n",
    "    \"\"\"TensorRT inference class for ViTPose model, optimized for GPU with TensorRT 10.8.0.43\"\"\"\n",
    "    def __init__(self, engine_path):\n",
    "        \"\"\"Initialize TensorRT engine and allocate buffers on GPU.\"\"\"\n",
    "        self.logger = trt.Logger(trt.Logger.INFO)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "\n",
    "        # Load engine from file\n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_bytes = f.read()\n",
    "            self.engine = self.runtime.deserialize_cuda_engine(engine_bytes)\n",
    "\n",
    "        if not self.engine:\n",
    "            raise RuntimeError(f\"Failed to load TensorRT engine from {engine_path}\")\n",
    "\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Define input and output shapes\n",
    "        self.input_shape = (1, 3, 256, 192)  # ViTPose standard input\n",
    "        self.output_shape = (1, 17, 64, 48)  # ViTPose standard output (17 keypoints)\n",
    "        \n",
    "        # Tensor names (adjust based on your engine)\n",
    "        self.input_name = \"input\"\n",
    "        self.output_name = \"output\"\n",
    "        \n",
    "        # Allocate GPU buffers\n",
    "        self.allocate_buffers()\n",
    "\n",
    "    def allocate_buffers(self):\n",
    "        \"\"\"Allocate CUDA memory for inputs and outputs.\"\"\"\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.bindings = []\n",
    "\n",
    "        num_io_tensors = self.engine.num_io_tensors\n",
    "        for i in range(num_io_tensors):\n",
    "            tensor_name = self.engine.get_tensor_name(i)\n",
    "            tensor_shape = self.engine.get_tensor_shape(tensor_name)\n",
    "            tensor_dtype = trt.nptype(self.engine.get_tensor_dtype(tensor_name))\n",
    "            \n",
    "            if tensor_dtype == np.float32:\n",
    "                torch_dtype = torch.float32\n",
    "            elif tensor_dtype == np.float16:\n",
    "                torch_dtype = torch.float16\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported dtype: {tensor_dtype}\")\n",
    "\n",
    "            # Explicitly allocate on GPU (CUDA)\n",
    "            tensor = torch.zeros(tuple(tensor_shape), dtype=torch_dtype, device='cuda')\n",
    "            self.bindings.append(tensor.data_ptr())\n",
    "            \n",
    "            if self.engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:\n",
    "                self.inputs.append(tensor)\n",
    "            else:\n",
    "                self.outputs.append(tensor)\n",
    "\n",
    "            self.context.set_tensor_address(tensor_name, tensor.data_ptr())\n",
    "\n",
    "    def preprocess_image(self, img):\n",
    "        \"\"\"Preprocess image for model input on GPU.\"\"\"\n",
    "        # Convert to RGB and resize on CPU first (cv2 doesn't support GPU natively)\n",
    "        img = cv2.resize(img, (192, 256))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = img.transpose(2, 0, 1)  # HWC to CHW\n",
    "        img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "        \n",
    "        # Move to GPU\n",
    "        return torch.from_numpy(img).to('cuda')\n",
    "\n",
    "    def postprocess_output(self, output):\n",
    "        \"\"\"Convert model output to keypoints on GPU, then move to CPU for further processing.\"\"\"\n",
    "        heatmaps = output.reshape(self.output_shape)\n",
    "        keypoints = {}\n",
    "        for batch_idx in range(heatmaps.shape[0]):\n",
    "            person_keypoints = torch.zeros((17, 3), device='cuda')  # Process on GPU\n",
    "            for kpt_idx in range(17):\n",
    "                heatmap = heatmaps[batch_idx, kpt_idx]\n",
    "                flat_idx = torch.argmax(heatmap)\n",
    "                y, x = torch.unravel_index(flat_idx, heatmap.shape)\n",
    "                orig_x = x * (192 / 48)  # Scale to input width\n",
    "                orig_y = y * (256 / 64)  # Scale to input height\n",
    "                confidence = heatmap[y, x]\n",
    "                person_keypoints[kpt_idx] = torch.tensor([orig_x, orig_y, confidence], device='cuda')\n",
    "            keypoints[batch_idx] = person_keypoints.cpu().numpy()  # Move to CPU for compatibility\n",
    "        return keypoints\n",
    "    def infer(self, img):\n",
    "        \"\"\"Run inference on an image using GPU.\"\"\"\n",
    "        preprocessed = self.preprocess_image(img)\n",
    "        self.inputs[0].copy_(preprocessed)\n",
    "        \n",
    "        # Execute inference asynchronously on GPU\n",
    "        self.context.execute_async_v3(stream_handle=torch.cuda.current_stream().cuda_stream)\n",
    "        torch.cuda.synchronize()  # Wait for GPU computation to finish\n",
    "        \n",
    "        output = self.outputs[0]  # Already on GPU\n",
    "        keypoints = self.postprocess_output(output)\n",
    "        return keypoints\n",
    "\n",
    "class HorseGaitMonitor:\n",
    "    def __init__(self, model_path, output_dir=\"monitoring_output\"):\n",
    "        \"\"\"Initialize the horse gait monitoring system with TensorRT on GPU.\"\"\"\n",
    "        self.model = TensorRTInference(model_path)\n",
    "        \n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        self.pose_dirs = {\n",
    "            \"standing\": os.path.join(output_dir, \"standing\"),\n",
    "            \"walking\": os.path.join(output_dir, \"walking\"),\n",
    "            \"sitting\": os.path.join(output_dir, \"sitting\"),\n",
    "            \"pawing\": os.path.join(output_dir, \"pawing\"),\n",
    "        }\n",
    "        for dir_path in self.pose_dirs.values():\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        self.prev_positions = None\n",
    "        self.movement_buffer = []\n",
    "        self.state_buffer = []\n",
    "        self.visualizer = CombinedVisualizer()\n",
    "\n",
    "    def calculate_angle(self, p1, p2, p3):\n",
    "        \"\"\"Calculate angle between three points (CPU-based for simplicity).\"\"\"\n",
    "        v1 = p1 - p2\n",
    "        v2 = p3 - p2\n",
    "        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        angle = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))\n",
    "        return angle\n",
    "\n",
    "    def detect_state(self, keypoints):\n",
    "        \"\"\"Detect the horse's current state based on keypoints.\"\"\"\n",
    "        movement_detected = False\n",
    "\n",
    "        for person_id, kp_array in keypoints.items():\n",
    "            leg_points = {\n",
    "                \"L_F_Hip\": kp_array[5][:2],\n",
    "                \"L_F_Knee\": kp_array[6][:2],\n",
    "                \"L_F_Paw\": kp_array[7][:2],\n",
    "                \"R_F_Hip\": kp_array[8][:2],\n",
    "                \"R_F_Knee\": kp_array[9][:2],\n",
    "                \"R_F_Paw\": kp_array[10][:2],\n",
    "                \"L_B_Hip\": kp_array[11][:2],\n",
    "                \"L_B_Knee\": kp_array[12][:2],\n",
    "                \"L_B_Paw\": kp_array[13][:2],\n",
    "                \"R_B_Hip\": kp_array[14][:2],\n",
    "                \"R_B_Knee\": kp_array[15][:2],\n",
    "                \"R_B_Paw\": kp_array[16][:2]\n",
    "            }\n",
    "\n",
    "            angles = {\n",
    "                \"left_front\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"L_F_Hip\"]),\n",
    "                    np.array(leg_points[\"L_F_Knee\"]),\n",
    "                    np.array(leg_points[\"L_F_Paw\"])\n",
    "                ),\n",
    "                \"right_front\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"R_F_Hip\"]),\n",
    "                    np.array(leg_points[\"R_F_Knee\"]),\n",
    "                    np.array(leg_points[\"R_F_Paw\"])\n",
    "                ),\n",
    "                \"left_back\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"L_B_Hip\"]),\n",
    "                    np.array(leg_points[\"L_B_Knee\"]),\n",
    "                    np.array(leg_points[\"L_B_Paw\"])\n",
    "                ),\n",
    "                \"right_back\": self.calculate_angle(\n",
    "                    np.array(leg_points[\"R_B_Hip\"]),\n",
    "                    np.array(leg_points[\"R_B_Knee\"]),\n",
    "                    np.array(leg_points[\"R_B_Paw\"])\n",
    "                )\n",
    "            }\n",
    "\n",
    "            if any(100 <= angle <= 130 for angle in [angles[\"left_front\"], angles[\"right_front\"]]):\n",
    "                return \"pawing\"\n",
    "            if all(angle < 90 for angle in angles.values()):\n",
    "                return \"sitting\"\n",
    "\n",
    "            paw_positions = np.array([\n",
    "                leg_points[\"L_F_Paw\"],\n",
    "                leg_points[\"R_F_Paw\"],\n",
    "                leg_points[\"L_B_Paw\"],\n",
    "                leg_points[\"R_B_Paw\"]\n",
    "            ])\n",
    "\n",
    "            if self.prev_positions is not None:\n",
    "                movements = np.linalg.norm(paw_positions - self.prev_positions, axis=1)\n",
    "                self.movement_buffer.append(np.mean(movements))\n",
    "                if len(self.movement_buffer) > 10:\n",
    "                    self.movement_buffer.pop(0)\n",
    "                movement_detected = np.mean(self.movement_buffer) > 5.0\n",
    "\n",
    "            self.prev_positions = paw_positions\n",
    "\n",
    "        current_state = \"walking\" if movement_detected else \"standing\"\n",
    "        self.state_buffer.append(current_state)\n",
    "        if len(self.state_buffer) > 15:\n",
    "            self.state_buffer.pop(0)\n",
    "        return max(set(self.state_buffer), key=self.state_buffer.count)\n",
    "\n",
    "    def draw_state_annotation(self, frame, state):\n",
    "        \"\"\"Draw state annotation on frame.\"\"\"\n",
    "        annotated_frame = frame.copy()\n",
    "        colors = {\n",
    "            \"standing\": (255, 0, 0),  # Blue\n",
    "            \"walking\": (0, 0, 255),   # Red\n",
    "            \"sitting\": (0, 255, 0),   # Green\n",
    "            \"pawing\": (255, 165, 0)   # Orange\n",
    "        }\n",
    "        cv2.putText(\n",
    "            annotated_frame,\n",
    "            f\"State: {state.upper()}\",\n",
    "            (10, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            colors.get(state, (128, 128, 128)),\n",
    "            2\n",
    "        )\n",
    "        return annotated_frame\n",
    "\n",
    "    def save_frame(self, frame, state, frame_count):\n",
    "        \"\"\"Save frame to appropriate directory based on state.\"\"\"\n",
    "        if state in self.pose_dirs:\n",
    "            filename = os.path.join(self.pose_dirs[state], f\"frame_{frame_count}.jpg\")\n",
    "            cv2.imwrite(filename, frame)\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        \"\"\"Process video file and analyze horse gait on GPU.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        combined_height = height + 150  # Add space for visualizer\n",
    "        output_path = os.path.join(self.output_dir, \"video_with_analysis2.mp4\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, combined_height))\n",
    "\n",
    "        frame_count = 0\n",
    "        last_announced_state = None\n",
    "\n",
    "        cv2.namedWindow(\"Horse Gait Analysis\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                keypoints = self.model.infer(frame)\n",
    "                current_state = self.detect_state(keypoints)\n",
    "                \n",
    "\n",
    "                if current_state != last_announced_state:\n",
    "                    print(f\"Frame {frame_count}: Horse is now {current_state}\")\n",
    "                    last_announced_state = current_state\n",
    "\n",
    "                annotated_frame = self.draw_state_annotation(frame, current_state)\n",
    "                self.save_frame(annotated_frame, current_state, frame_count)\n",
    "\n",
    "                self.visualizer.update(current_state)\n",
    "                combined_display = self.visualizer.create_visualization(annotated_frame)\n",
    "\n",
    "                cv2.imshow(\"Horse Gait Analysis\", combined_display)\n",
    "                out.write(combined_display)\n",
    "\n",
    "                frame_count += 1\n",
    "                if frame_count % 30 == 0:\n",
    "                    progress = (frame_count / total_frames) * 100\n",
    "                    print(f\"Processing progress: {progress:.1f}%\")\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "\n",
    "        finally:\n",
    "            cap.release()\n",
    "            out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(f\"\\nProcessing complete! Output saved to: {output_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the horse gait analysis on GPU.\"\"\"\n",
    "    engine_path = \"C:/Users/hp/Downloads/vitpose-l-ap10k.engine\"\n",
    "    try:\n",
    "        # Print GPU info\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        monitor = HorseGaitMonitor(engine_path)\n",
    "        monitor.process_video(\"E:/vitpose/strach.mp4\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d59d5-e85d-4383-9146-85c66fb2aa83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cee07-f62e-489c-a264-95e4eebc6f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b793a5-9d61-4b79-ab9f-0e51c4f3639d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f29e0-769c-439b-8a1f-a26c90733fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce1b25-31a2-43fc-a244-c4dd4f983bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c94521-a74c-4f71-8b36-c990c001b047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7827d81-8251-45e1-a063-9d5edc1b5df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32a7c2-3122-4047-ae2e-5780ccdef4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc051e-6e09-4d40-95e7-2247c05eac67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc0bbc-975f-4445-b63b-80f0f86268b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "339ac911-1e7a-4019-b234-cb2f1e565c25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautoinit\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01measy_ViTPose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01measy_ViTPose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvit_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m engine \u001b[38;5;28;01mas\u001b[39;00m engine_utils \u001b[38;5;66;03m# TRT Engine creation/save/load utils\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[1;32mE:\\easy_ViTPose\\easy_ViTPose\\vit_utils\\engine.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvit_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelData\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# ../../common.py\u001b[39;00m\n\u001b[0;32m     14\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;241m1\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;18m__file__\u001b[39m)), os\u001b[38;5;241m.\u001b[39mpardir, os\u001b[38;5;241m.\u001b[39mpardir)\n\u001b[0;32m     16\u001b[0m )\n",
      "File \u001b[1;32mE:\\easy_ViTPose\\easy_ViTPose\\vit_utils\\model.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolygraphy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EngineFromNetwork, NetworkFromOnnxPath\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolygraphy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m G_LOGGER\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PATHS\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelData\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Contains converted model metadata\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from easy_ViTPose.easy_ViTPose.vit_utils import engine as engine_utils # TRT Engine creation/save/load utils\n",
    "\n",
    "from time import time\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from vit_utils.visualization import draw_points_and_skeleton, joints_dict\n",
    "from vit_utils.dist_util import get_dist_info, init_dist\n",
    "from vit_utils.top_down_eval import keypoints_from_heatmaps\n",
    "\n",
    "TRT_PATH = \"C:/Users/hp/Downloads/vitpose-l-ap10k.engine\"\n",
    "\n",
    "# SETUP TRT\n",
    "logger = trt.Logger(trt.Logger.ERROR)\n",
    "trt_runtime = trt.Runtime(logger)\n",
    "\n",
    "print(\"Loading cached TensorRT engine from {}\".format(TRT_PATH))\n",
    "trt_engine = engine_utils.load_engine(trt_runtime, TRT_PATH)\n",
    "\n",
    "# This allocates memory for network inputs/outputs on both CPU and GPU\n",
    "inputs, outputs, bindings, stream = engine_utils.allocate_buffers(trt_engine)\n",
    "\n",
    "# Execution context is needed for inference\n",
    "context = trt_engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b30c5b1c-1431-4746-99c1-fafd17f00a0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m img_input \u001b[38;5;241m=\u001b[39m img_input\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Copy it into appropriate place into memory\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# (inputs was returned earlier by allocate_buffers())\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m np\u001b[38;5;241m.\u001b[39mcopyto(\u001b[43minputs\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhost, img_input\u001b[38;5;241m.\u001b[39mravel())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Feed to model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m tic \u001b[38;5;241m=\u001b[39m time()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare input data\n",
    "IMG_PATH = \"E:/lame.webp\"\n",
    "img = cv2.cvtColor(cv2.imread(IMG_PATH), cv2.COLOR_BGR2RGB)\n",
    "org_h, org_w = img.shape[:2]\n",
    "img_input = cv2.resize(img, (192, 256), interpolation=cv2.INTER_LINEAR)\n",
    "img_input = img_input.astype(np.float32).transpose(2, 0, 1)[None, ...] / 255\n",
    "\n",
    "# Copy it into appropriate place into memory\n",
    "# (inputs was returned earlier by allocate_buffers())\n",
    "np.copyto(inputs[0].host, img_input.ravel())\n",
    "\n",
    "# Feed to model\n",
    "tic = time()\n",
    "bs = 1\n",
    "# Fetch output from the model\n",
    "heatmaps = engine_utils.do_inference(\n",
    "    context, bindings=bindings, inputs=inputs,\n",
    "    outputs=outputs, stream=stream)[0]\n",
    "heatmaps = heatmaps.reshape((1, 25, 64, 48))\n",
    "\n",
    "elapsed_time = time()-tic\n",
    "print(f\">>> Output size: {heatmaps.shape} ---> {elapsed_time:.4f} sec. elapsed [{elapsed_time**-1: .1f} fps]\\n\")    \n",
    "\n",
    "points, prob = keypoints_from_heatmaps(heatmaps=heatmaps, center=np.array([[org_w//2, org_h//2]]), scale=np.array([[org_w, org_h]]),\n",
    "                                        unbiased=True, use_udp=True)\n",
    "points = np.concatenate([points[:, :, ::-1], prob], axis=2)\n",
    "\n",
    "# Visualization \n",
    "for pid, point in enumerate(points):\n",
    "    img = draw_points_and_skeleton(img.copy(), point, joints_dict()['coco']['skeleton'], person_index=pid,\n",
    "                                    points_color_palette='gist_rainbow', skeleton_color_palette='jet',\n",
    "                                    points_palette_samples=10, confidence_threshold=0.4)\n",
    "    \n",
    "    plt.figure(figsize=(5,10))\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Result\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
